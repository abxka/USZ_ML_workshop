{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part B: Multi-layer perceptron for diabetes classification\n",
    "\n",
    "In this section you will learn how to implement a simple neural network in tensorflow and use it to classify whether a patient has diabetes or not based on a number of predictors such as BMI, Insulin levels, etc. \n",
    "\n",
    "### Neural networks biological motivation\n",
    "\n",
    "Not surprisingly there is an analogy between real biological neurons and the neurons used in deep learning. \n",
    "\n",
    "Biological Neuron | Computational Neuron\n",
    "- | - \n",
    "<img src=\"../images/neuron.png\" style=\"height:180px\" /> | <img src=\"../images/neuron_model.jpeg\" style=\"height:180px\" />\n",
    "\n",
    "In both cases a number of inputs get aggregated in a \"cell body\". In the biological neuron the output axon fires if the input signals cross a certain threshold. Similarly, the output of computational neurons is modulated by an **activation function**. Most commonly used activation functions go to 0 below a threshold commonly called the bias $b$ and have values greater than 0 above that threshold. \n",
    "\n",
    "For the activation function sometimes a [**sigmoid**](https://en.wikipedia.org/wiki/Sigmoid_function) function is used. As we will see in part D of the tutorial, these days more commonly the [**rectified linear unit (ReLU)**](https://en.wikipedia.org/wiki/Rectifier_(neural_networks)) is used. However, in this part of the tutorial we will exclusively use the sigmoid activation denoted by $\\sigma(\\cdot)$. \n",
    "\n",
    "<img src=\"../images/activation_functions.png\" style=\"width: 700px;\"/> \n",
    "\n",
    "It is important to know that the analogies between computational neural networks and brains are a bit fanciful. In reality we don't really know how brains work. Deep computational neural networks are very crude approximations of biology and it is impossible to gain actual insights into how brains work from them. \n",
    "\n",
    "Let us start by loading the necessary packages and setting up the GPU environment. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "config=tf.ConfigProto()\n",
    "config.gpu_options.allow_growth=True\n",
    "config.allow_soft_placement=True\n",
    "\n",
    "import numpy as np\n",
    "import os\n",
    "import sys\n",
    "import matplotlib.pyplot as plt\n",
    "import math\n",
    "\n",
    "# This is necessary to load modules form the lib folder of the project\n",
    "module_path = os.path.abspath(os.path.join('..', 'lib'))\n",
    "if module_path not in sys.path:\n",
    "    sys.path.append(module_path)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The dataset we will be working with is originally from the National Institute of Diabetes and Digestive and Kidney Diseases. The objective of the dataset is to diagnostically predict whether or not a patient has diabetes, based on certain diagnostic measurements included in the dataset. Several constraints were placed on the selection of these instances from a larger database. In particular, all patients here are females at least 21 years old of Pima Indian heritage. \n",
    "\n",
    "We prepared the data as a python module which you can simply import:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import diabetes_data as data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In a next step, let us visualise a histogram of the 8 different input features and the diabetes of the data. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "table = np.concatenate([data.X_raw, data.y_raw[:, None]], axis=-1)\n",
    "\n",
    "feature_names = ('Pregnancies', 'Glucose', 'Blood Pressure',\n",
    "                 'SkinThickness', 'Insulin', 'BMI',\n",
    "                'DiabetesPedigreeFunction', 'Age', 'Has Diabetes?')\n",
    "plt.subplots(figsize=(18,15))\n",
    "for i in range(3):\n",
    "    for j in range(3):\n",
    "        plt.subplot(3,3,i*3+j + 1)\n",
    "        plt.subplots_adjust(wspace=0.2,hspace=0.5)\n",
    "        plt.hist(table[:, i*3+j], bins=20)\n",
    "        plt.title(feature_names[i*3+j])\n",
    "        \n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Furthermore, let's investigate how large the dataset is:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "print('The dataset has %d patients' % table.shape[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Actually, this is not a big dataset as far as machine learning problems go, but it will serve as a good introduction example. We will see a much bigger dataset in part D of this tutorial. \n",
    "\n",
    "Next, let us look at the theory of simple neural networks starting with the simplest of them all. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Logistic regression: the simplest neural network\n",
    "\n",
    "The image below shows the schematic of a single neuron with inputs $x_i$, weights $w_i$ and a bias term $b$. \n",
    "\n",
    "For a classification problem with two classes (i.e. a binary classification) the output of a single neuron can be written in matrix form as \n",
    "\n",
    "$$ h(x) = \\sigma(w^Tx + b), $$ \n",
    "\n",
    "where $w = (w_1, w_2, ..., w_D)^T$, $x = (x_1, x_2, ..., x_D)^T$ and $\\sigma(z) = 1 / (1 + e^{-z})$ the sigmoid activation function.\n",
    "\n",
    "This is the widely used **logistic regression classifier** that you saw in the Machine Learning I tutorial yesterday. The output $y \\in (0,1)$ can be interpreted as the probability of the inputs $x$ corresponding to class 1 (rather than class 0). \n",
    "\n",
    "We can easily build this model in tensorflow. Knowing that our input data has 8 features $x_1, ..., x_8$, and one output, what should the shapes of the vectors $w$ and $b$ be?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "X_pl = tf.placeholder(dtype=tf.float32, shape=[None, 8], name='X') # Note that a shape dimension of None means \n",
    "y_pl = tf.placeholder(dtype=tf.float32, shape=[None, ], name='y')  # that we can give any size to this placeholder later\n",
    "\n",
    "### IMPLEMENT THE FOLLOWING ###\n",
    "# The weights need to be (something x 1 matrices), not vectors because we use the tf.matmul matrix multiplication function. I.e. \n",
    "# the shape should be something of the form: (a, b). \n",
    "# shape_of_w =  ...\n",
    "# shape_of_b =  ...\n",
    "################################\n",
    "\n",
    "w = tf.Variable(tf.truncated_normal(shape_of_w, stddev=math.sqrt(2.0 / float(8)))) # We initialise the weights randomly                                                                                   # with a normal distribution\n",
    "b = tf.Variable(tf.zeros(shape_of_b))\n",
    "\n",
    "l_out = tf.matmul(X_pl, w) + b\n",
    "y_out = tf.sigmoid(l_out)\n",
    "\n",
    "hard_pred_out = tf.round(y_out)  # Rounding the output probability gives a hard 0 prediction if y_out<0.5 \n",
    "                                 # and a 1 prediction if y_out >= 0.5"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that we initialised the weights $w$ with a random distribution with a specific standard deviation (stddev). There are some rules for choosing a good standard deviation to facilitate the learning, we will see a strategy called Xavier initialisation further down. However, for this simple problem almost any distribution should work. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training a logistic regression classifier \n",
    "\n",
    "The weights $w$ and the bias $b$ are the **parameters** of the model. Given a training set of inputs $x^{(n)}$ and ground truth labels $y^{(n)}$ the objective is to find values for $w$ and $b$ that map the network output $h(x^{(n)})$ for each input $x^{(n)}$ to the correct $y^{(n)}$ as well as possible. Let us collectively refer to all parameters as $\\phi = (w,b)$ to simplify notation. This is usually done by minimising the **cross entropy loss** which is defined as:\n",
    "\n",
    "$$ J(\\phi) = -\\sum_{n=1}^N y^{(n)} \\log\\left(h(x^{(n)})\\right) + (1-y^{(n)})\\log\\left(1-h(x^{(n)})\\right) $$\n",
    "\n",
    "This loss comes from signal processing. To understand it intuitively you can think about what happens if the ground-truth label is $y^{(n)}=0$ and what happens if it is $y^{(n)}=1$ and what values the log function can take. However, going into the details of this loss beyond the scope of this tutorial. It is enough to know that the values of $w,b$ that produce the smallest value of $J$ will lead to a in some sense optimal classification.  \n",
    "\n",
    "Note that the sum above goes over all training pairs $\\{x^{(n)}, y^{(n)} \\}$. However, often when working with huge datasets we cannot load the entire dataset at once because of memory restrictions. Instead the data is randomly split into smaller **mini batches**. For each mini batch a simple gradient descent step is performed to move the parameters $\\phi$ in the direction of decreasing loss: \n",
    "\n",
    "$$ \\phi_{t+1} = \\phi_t - \\lambda \\frac{\\partial}{\\partial \\phi_t} J(\\phi_t) $$ \n",
    "\n",
    "This procedure is referred to as **mini batch gradient descent**.\n",
    "\n",
    "`tensorflow` provides handy functions to implement all of this. Specifically, we almost always use the `tf.nn.sigmoid_cross_entropy_with_logits` function to implement the loss. This function expects the ground-truth labels (`y_pl`, in our case) and the output **logits**, i.e. the thing *before* the sigmoid function for computational reasons. When we defined the model above we defined it as `l_out`. \n",
    "\n",
    "We can also define an **optimiser** using `tensorflow`. Below, we chose the `tf.train.AdamOptimizer` with a learning rate of $\\lambda=0.005$. Adam is a very powerful mini batch gradient descent optimizer that tends to produce good results without much tuning. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Define the loss\n",
    "loss = tf.nn.sigmoid_cross_entropy_with_logits(labels=tf.expand_dims(y_pl, axis=-1), logits=l_out) \n",
    "                # The `tf.expand_dims` function above is needed to \n",
    "                # make the dimensions of the two tensorflow objects match. \n",
    "        \n",
    "# Define the optimiser and a training operation that performs a single mini batch gradient descent step\n",
    "optimiser = tf.train.AdamOptimizer(learning_rate=0.005)  \n",
    "train_op = optimiser.minimize(loss)\n",
    "\n",
    "# Define the initialisation operation that, when run in the session, will assign the initial values to w and b\n",
    "init = tf.global_variables_initializer()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let us also define two helper functions to validate the model performance during training and to evaluate the final performance of the classifier after training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def validate(sess, data_fold, loss_pl, X_pl, y_pl):\n",
    "    \n",
    "    total_loss = 0\n",
    "    total_points = 0\n",
    "    for val_batch in data_fold.iterate_batches(1):\n",
    "        X_b, y_b = val_batch\n",
    "        total_loss += sess.run(loss_pl, feed_dict={X_pl: X_b, y_pl: y_b})[0]\n",
    "        total_points += 1\n",
    "        \n",
    "    return total_loss / total_points\n",
    "\n",
    "from sklearn.metrics import classification_report, f1_score, confusion_matrix, accuracy_score\n",
    "def evaluate_test_set(sess, X_pl, hard_pred_out):\n",
    "\n",
    "    predictions_eval = sess.run(hard_pred_out, feed_dict={X_pl: data.X_test})\n",
    "\n",
    "    print('Classification Report:')\n",
    "    print(classification_report(data.y_test, predictions_eval, target_names=['Diabetes', 'No Diabetes']))\n",
    "\n",
    "    print('Confusion Matrix:')\n",
    "    print(confusion_matrix(data.y_test, predictions_eval, labels=[0,1]))\n",
    "\n",
    "    print('Accuracy: %.2f percent' % (accuracy_score(data.y_test, predictions_eval)*100))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To simplify the mini batch learning the datasets we use in this tutorial already have functions built-in to slice up the data into mini batches of a desired size. The example below, slices the training data into batches of size 10 and iterates through the whole dataset once (ignoring the last batch that will no longer have 10 elements). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "counter = 0\n",
    "for batch in data.train.iterate_batches(batch_size=10):\n",
    "    Xb, yb = batch\n",
    "    print('Shape of batch #%d:' % counter)\n",
    "    print(Xb.shape)\n",
    "    print(yb.shape)\n",
    "    counter += 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following loop executes the `train_op` we defined above multiple times for different mini batches. The inner loop goes over the dataset once as in the example above. Hence the outer loop repeats this 50 times. One whole pass over the training data is often called an **epoch**. This means we train the model for 50 epochs. In the code below, execute the training operation defined above in the tf.Session(). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "step = 0\n",
    "train_losses = []\n",
    "val_losses = []\n",
    "\n",
    "sess = tf.Session(config=config)\n",
    "sess.run(init)\n",
    "    \n",
    "for epoch in range(50):  \n",
    "\n",
    "    for batch in data.train.iterate_batches(10):\n",
    "\n",
    "        X_b, y_b = batch\n",
    "        \n",
    "        ### RUN THE TRAINING OPERATION HERE ###\n",
    "        # Remember to feed to values for the X and y of the current batch\n",
    "        # ...\n",
    "        #######################################\n",
    "\n",
    "        if step % 50 == 0: \n",
    "            \n",
    "            val_loss = validate(sess, data.validation, loss, X_pl, y_pl)\n",
    "            train_loss = validate(sess, data.train, loss, X_pl, y_pl)\n",
    "            \n",
    "            val_losses.append(val_loss)\n",
    "            train_losses.append(train_loss)\n",
    "\n",
    "        if step % 500 == 0:\n",
    "\n",
    "            print('Step: %d (epoch %d)' % (step, epoch))\n",
    "            print(' - Validation Loss: %f' % val_loss)\n",
    "            print(' - Training Loss: %f' % train_loss)\n",
    "\n",
    "        step += 1\n",
    "      \n",
    "plt.plot(range(len(train_losses)), train_losses, 'b-',\n",
    "         range(len(val_losses)), val_losses, 'g-')\n",
    "plt.legend(['Training loss','Validation loss'])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we can evaluate the performance of this classifier using the function we defined above. Precision and recall measure the systems ability to correctly classify diabetes cases, and the percentage of correctly retrieved diabetes cases, respectively. The f1 score is an average between the two. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "evaluate_test_set(sess, X_pl, hard_pred_out)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that something very similar is happening \"under the hood\" if you run the `sklearn` logistic regression classifier. However, to build state-of-the-art deep learning architectures the additional flexibility gained by implementing this ourselves using `tensorflow` is very useful. Let us now build a more complicated network using the same approach.\n",
    "\n",
    "### Multi-layer perceptrons. \n",
    "\n",
    "The basic neuron unit from above can be arranged in layers to form a multi-layer neural network:\n",
    "\n",
    "<img src=\"../images/mlp.png\" style=\"height:230px\"/>\n",
    "\n",
    "The example above shows a network with 2 input features, 2 hidden layers with 4 and 3 neurons, respectively, and one output. In the last layer we almost always use a sigmoid function for binary classification, in the intermediate layers other activation functions such as the ReLU can be used, however, here we will also use the sigmoid. Note that the last layer is, in fact, exactly a logistic regression classifier with 3 inputs. \n",
    "\n",
    "Each of the arrows has a weight value attached to it. Let's focus only on the first layer for now. Since every unit of the input is connected with every unit of the first hidden layer the weights can be expressed as a 2D matrix. The activation of the each neuron is calculated described in the last section. This can be conveniently calculated by a matrix multiplication. \n",
    "\n",
    "$a^{(1)} = \\sigma(W^{(1)}x + b^{(1)}) \\quad\n",
    "\\mbox{with}\n",
    "\\quad   \n",
    "W^{(1)} = \\begin{bmatrix} \n",
    " w_{11} & w_{12} & w_{13} & w_{14} \\\\\n",
    " w_{21} & w_{22} & w_{23} & w_{24}\n",
    "\\end{bmatrix} \\quad \\mbox{and} \\quad b^{(1)} = \\begin{bmatrix} \n",
    " b_1 \\\\\n",
    " b_2 \n",
    " \\end{bmatrix} $\n",
    " \n",
    "The activation $a^{(1)}$ then serves as the input to the next layer and so on. \n",
    "\n",
    "Using the template below extend the logistic regression classifier model from above to a 2 hidden layer neural network with 10 neurons in the first layer, and 5 neurons in the second hidden layer. Use the following formula to set the standard deviation of the initial distribution of each weight matrix:\n",
    "\n",
    "$$ stddev_W = \\sqrt{\\frac{2}{(\\#\\,inputs) + (\\#\\,outpus)}} $$\n",
    "\n",
    "Initialising weights with a normal distribution with this standard deviation is called **Xavier initialisation**. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "X_pl = tf.placeholder(tf.float32, [None, 8], name='X')\n",
    "y_pl = tf.placeholder(tf.float32, [None, ], name='y')\n",
    "\n",
    "input_size = 8\n",
    "num_hidden_1 = 10\n",
    "num_hidden_2 = 5\n",
    "output_size = 1 \n",
    "\n",
    "### IMPLMENT AN MLP USING THE EQUATIONS ABOVE ###\n",
    "# You will need to define variables W1, W2, W3, b1, b2, b3 and evaluate the \n",
    "# activations a1, a2 using the equations above. The last logits (value before activation)\n",
    "# and activation are already implemented and denoted as `l_out` and `y_out`. \n",
    "\n",
    "# ...\n",
    "\n",
    "###################################################\n",
    "\n",
    "l_out = tf.matmul(a2, W3) + b3\n",
    "y_out = tf.sigmoid(l_out)\n",
    "\n",
    "hard_pred_out = tf.round(y_out)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The loss and optimisation are exactly the same as in the logistic regression example:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "loss = tf.nn.sigmoid_cross_entropy_with_logits(labels=tf.expand_dims(y_pl, axis=-1), logits=l_out)\n",
    "optimiser = tf.train.AdamOptimizer(0.005)\n",
    "train_op = optimiser.minimize(loss)\n",
    "init = tf.global_variables_initializer()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Also the training loop is exactly the same as before. Go ahead and run the training. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "step = 0\n",
    "train_losses = []\n",
    "val_losses = []\n",
    "\n",
    "sess = tf.Session(config=config)\n",
    "sess.run(init)\n",
    "    \n",
    "for epoch in range(50): \n",
    "\n",
    "    for batch in data.train.iterate_batches(10):\n",
    "\n",
    "        X_b, y_b = batch\n",
    "        \n",
    "        sess.run(train_op, feed_dict={X_pl: X_b, y_pl: y_b})\n",
    "\n",
    "        if step % 50 == 0: \n",
    "            \n",
    "            val_loss = validate(sess, data.validation, loss, X_pl, y_pl)\n",
    "            train_loss = validate(sess, data.train, loss, X_pl, y_pl)\n",
    "            \n",
    "            val_losses.append(val_loss)\n",
    "            train_losses.append(train_loss)\n",
    "\n",
    "        if step % 500 == 0:\n",
    "\n",
    "            print('Step: %d (epoch %d)' % (step, epoch))\n",
    "            print(' - Validation Loss: %f' % val_loss)\n",
    "            print(' - Training Loss: %f' % train_loss)\n",
    "\n",
    "        step += 1\n",
    "\n",
    "plt.plot(range(len(train_losses)), train_losses, 'b-',range(len(val_losses)), val_losses, 'g-')\n",
    "plt.legend(['Training loss','Validation loss'])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "evaluate_test_set(sess, X_pl, hard_pred_out)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You should have hopefully observed an improvement in classification accuracy with respect to the logistic regression classifier of 2-3 percent or more. The exact value can vary randomly due to the random initialisation of the weights. If you want you can rerun the training and see if you randomly get a slightly better or worse result. \n",
    "\n",
    "Otherwise, close the Session and move on to part C of the tutorial "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "sess.close()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
