{"nbformat":4,"nbformat_minor":0,"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.6.10"},"colab":{"name":"C_Convolutions.ipynb","provenance":[],"collapsed_sections":[]}},"cells":[{"cell_type":"markdown","metadata":{"id":"_rTqrHuk5tuu"},"source":["## PART C: Convolutions in tensorflow\n","\n","In this section we cover the basics of convolutions and will learn how to apply convolutions to an image using tensorflow. We will do this by implementing the [Sobel filter](https://en.wikipedia.org/wiki/Sobel_operator), which highlights edges in images. In the next step, we will try to optimise a set of convolutional filters such that they produce the same output as the Sobel filter. "]},{"cell_type":"markdown","metadata":{"id":"tJkPTiHw6Jdt"},"source":["<h2> <u>Mount Google drive folder</u> </h2>\n"]},{"cell_type":"code","metadata":{"id":"_Tm78uag6J7N"},"source":["# Mount Google drive\n","from google.colab import drive\n","drive.mount('/content/drive')"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"ZoGelIv66btL"},"source":["cd /content/drive/My Drive/ML_workshop"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"bry7hn4n6dGz"},"source":["ls"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"collapsed":true,"id":"PKS8RTJZ5tuz"},"source":["# Import the required libraries\n","import tensorflow.compat.v1 as tf\n","tf.disable_v2_behavior()\n","\n","config=tf.ConfigProto()\n","config.gpu_options.allow_growth=True\n","config.allow_soft_placement=True\n","\n","import numpy as np\n","import matplotlib.pyplot as plt\n","\n","# Import acdc_data (need to add the lib package to the system path)\n","import os\n","import sys\n","module_path = os.path.abspath(os.path.join('.', './deep_learning/lib'))\n","\n","if module_path not in sys.path:\n","    sys.path.append(module_path)\n","    \n","# Now we can import the module\n","import acdc_data"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"6TBI87005tu0"},"source":["First, let's load a random 2D slice from the [ACDC Cardiac Segmentation Challenge](https://www.creatis.insa-lyon.fr/Challenge/acdc/) dataset and displaying it. "]},{"cell_type":"code","metadata":{"collapsed":true,"id":"_ABmHvL45tu0"},"source":["# Test plotting stuff\n","\n","img, mask = acdc_data.train.next_batch(batch_size=1)\n","img_disp = np.squeeze(img)  # The first dimensions contains the batch size, so we squeeze it away. \n","\n","plt.figure(figsize=(6, 6))\n","plt.imshow(img_disp, cmap='gray')\n","plt.show()\n","\n","# If you don't like the image you can rerun the block"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"_t6c68Oe5tu0"},"source":["### 2D Convolutions \n","\n","Convolution is a very commonly used operation in signal processing. It let's us calculate the response of a **signal** to a **filter**. When dealing with images the signal ($f$) is the image and the filter ($h$) is a 2D array. The discrete 2D convolution is defined as:\n","\n","$$ (f*h)[x,y] = \\sum_{n_1} \\sum_{n_2} f[n_1, n_2] h[x-n_1, y-n_2] $$\n","\n","It can be easily understood graphically. The filter (dark blue) is shifted over the input image (blue), the kernel values and image values are multiplied at the current kernel position, summed up and placed into the output image (green) at that location. \n","\n","<img src=\"../images/full_padding_no_strides_transposed.gif\" style=\"width: 250px;\"/> \n","\n","(This animation was downloaded [here](https://github.com/vdumoulin/conv_arithmetic))"]},{"cell_type":"markdown","metadata":{"id":"z8sd24el5tu1"},"source":["The Sobel filter calculates an approximation to the magnitude of the gradient of the image. More precisely, the image's intensity function. The gradients $G_x$, $G_y$ in $x$ and $y$ directions can be approximated by convolving the image with specific convolutional kernels. Namely:\n","\n","$ G_x = \\begin{bmatrix} \n"," +1 & 0 & -1  \\\\\n","+2 & 0 & -2 \\\\\n","+1 & 0 & -1 \n","\\end{bmatrix} * f\n","\\quad\n","\\mbox{and}\n","\\quad   \n","G_y = \\begin{bmatrix} \n"," +1 & +2 & +1\\\\\n"," 0 & 0 & 0 \\\\\n","-1 & -2 & -1\n","\\end{bmatrix} * f $\n","\n","The sobel filtered image is then given by the magnitude of the gradient:\n","\n","$ S = \\sqrt{G_x^2 + G_y^2} $ \n","\n","Next, we will create an expression that let's us filter an input image placeholder with placeholders for convolutional filters. \n","\n","Tensorflow provides us with a function to perform 2D convolutions called `tf.nn.conv2d`. The function operates on **4D tensors** assuming the following shape: `[batch_size, image_size_x, image_size_y, num_color_channels]`. Here is the syntax:\n","\n","```python\n","conv2d(\n","    input,\n","    filter,\n","    strides,\n","    padding,\n","    use_cudnn_on_gpu=None,\n","    data_format=None,\n","    name=None\n",")\n","```\n","\n","We see that the function requires four mandatory parameters: \n"," - **input**: image placeholder, \n"," - **filter**: filter weights placeholder/variable. Should have the following shape `[kernel_size_x, kernel_size_y, input_dimensions, output_dimensions]`. \n"," - **strides**: numpy array defining the size of steps to make in each direction. For us this should be `[1,1,1,1]`.\n"," - **padding**: this option defines how the operation deals with the borders. If set to `'VALID'` the output image shrinks because the convolutional filter cannot be evaluated at the image borders. If set to `SAME` the input image will be padded with zeros such that the output size is the same as the input size. \n"," \n","Below, implement the expressions for `G_x_pl` and `G_y_pl`. Use `padding='VALID'` for now, but `SAME` would work just as well. Furthermore, evaluate the two expressions in a session, and plot the results. "]},{"cell_type":"code","metadata":{"collapsed":true,"id":"V7FB8Smp5tu1"},"source":["img_pl = tf.placeholder(tf.float32)\n","h_x_pl = tf.placeholder(tf.float32)\n","h_y_pl = tf.placeholder(tf.float32)\n","\n","### IMPLEMENT TENSORFLOW EXPRESSION FOR G_x_pl and G_y_pl ###\n","# G_x_pl = ...\n","# G_y_pl = ...\n","#############################################################\n","\n","H_x = np.array([[-1,  0, 1],\n","              [-2,  0, 2],\n","              [-1,  0, 1]])\n","H_x = np.reshape(H_x, (3, 3, 1, 1)) \n","\n","H_y = np.array([[-1, -2, -1],\n","              [  0,  0, 0],\n","              [  1,  2, 1]])\n","H_y = np.reshape(H_y, (3, 3, 1, 1))  \n","\n","F = np.expand_dims(img, axis=-1)  # add the missing color channel\n","\n","with tf.Session(config=config) as sess:\n","    \n","    ### EVALUATE G_x_eval, G_y_eval USING THE TF.SESSION ###\n","    # Remember that you can evaluate multiple tensorflow objects simultaneously\n","    # by using sess.run([a,b], feed_dict={ \"required inputs\" })\n","    # Don't forget to feed the values the actual filter values and the input image to the expression \n","    \n","    [G_x_eval, G_y_eval] = sess.run([G_x_pl, G_y_pl], feed_dict={img_pl: F,\n","                                                                 h_x_pl: H_x, \n","                                                                 h_y_pl: H_y})   # implement this\n","    \n","    ########################################################\n","    \n","# Display the result\n","plt.figure(figsize=(12,6))\n","plt.subplot(1,3,1)\n","plt.imshow(np.squeeze(img), cmap='gray')\n","plt.subplot(1,3,2)\n","plt.imshow(np.squeeze(G_x_eval), cmap='gray')\n","plt.subplot(1,3,3)\n","plt.imshow(np.squeeze(G_y_eval), cmap='gray')\n","plt.show()\n","\n","print('The input image was of shape:')\n","print(np.squeeze(img).shape)\n","print('The output images have the following shape')\n","print(np.squeeze(G_x_eval).shape)  # One pixel at each border got lost"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"jeMCsbnn5tu2"},"source":["Now create an expression for the Sobel filtered image ($ S = \\sqrt{G_x^2 + G_y^2} $ ) and evalate that expression. You will need the `tf.sqrt` function."]},{"cell_type":"code","metadata":{"collapsed":true,"id":"isWfebAv5tu2"},"source":["### IMPLEMENT THIS ###\n","# sobel_pl = ...\n","######################\n","\n","with tf.Session(config=config) as sess:\n","\n","    sobel_eval = sess.run(sobel_pl, feed_dict={img_pl: F, h_x_pl: H_x, h_y_pl: H_y})\n","\n","# Display the result\n","plt.figure(figsize=(10,6))\n","plt.subplot(1,2,1)\n","plt.imshow(np.squeeze(img), cmap='gray')\n","plt.subplot(1,2,2)\n","plt.imshow(np.squeeze(sobel_eval), cmap='gray')\n","plt.show()"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"zQwGQ_kB5tu3"},"source":["### Learning the parameters of convolutional filters\n","\n","Now that we successfully created a Sobel filtered image, let's try a different approach. Suppose someone gave you the two images above without telling you anything about how the one on the right was generated. Could we optimise the parameters of our convolutional filters until they perform the same operation. That is, can be learn the Sobel filtering operation?\n","\n","Let's start by generating Filter **Variables**, rather than placeholders. Recall that the value of variables can be optimised as opposed to placeholders which have no value, and numpy arrays which are not part of tensorflow computational graphs. In the example below note, that we don't initialise the Variables as a constant, but we initialise with random numbers from a random distribution with a standard deviation of one. In this example, initialisation doesn't have a huge impact. However, in deep convolutional neural networks such as the one in part E of this practical, or even in the smaller multi layer perceptron in part B, the initial distribution of the variables plays a significant role. \n","\n","In addition to the input image placeholder, now, we will also need a placeholder for the target image (i.e. the Sobel filtered output). \n"]},{"cell_type":"code","metadata":{"collapsed":true,"id":"FFJEG8Wt5tu3"},"source":["tf.reset_default_graph()\n","sess.close()\n","\n","h_x_var = tf.Variable(tf.truncated_normal([3, 3, 1, 1], stddev=0.1, name='filter_x'))\n","h_y_var = tf.Variable(tf.truncated_normal([3, 3, 1, 1], stddev=0.1, name='filter_y'))\n","\n","img_pl = tf.placeholder(tf.float32)  # already defined above, but repeated here for clarity\n","target_pl = tf.placeholder(tf.float32)  # Will hold the ground-truth Sobel filtered image"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"ra9e1-xz5tu3"},"source":["Also, let us redefine the Sobel filter operation with the filter **variables**. "]},{"cell_type":"code","metadata":{"collapsed":true,"id":"8bQ9M5uj5tu3"},"source":["# Redefine the expressions for the filtered images and the Sobel filter output using the newly defined filter Variables\n","G_x_pl = tf.nn.conv2d(img_pl, h_x_var, strides=(1,1,1,1), padding='VALID')\n","G_y_pl = tf.nn.conv2d(img_pl, h_y_var, strides=(1,1,1,1), padding='VALID')\n","sobel_pl = tf.sqrt(G_x_pl**2 + G_y_pl**2)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"X8IuCX_S5tu3"},"source":["We want to find a set of filters `h_x_var` and `h_y_var` that produce the same output as the Sobel filter from above. In order to solve this problem we first need to define what a good/bad network output is. This means we first need to define a loss function. Since we want the Sobel filtered image `sobel_pl` to be as similar as possible to the target (or ground-truth) Sobel filtered image from above `target_pl`, we can simply use the mean squared error (MSE) loss which is defined as: \n","\n","$ L(f,g) = \\frac{1}{N} \\sum_{i,j} (f_{ij}-g_{ij})^2, $\n","\n","where $N$ is the number of pixels in the image. The mean of all elements of a tensor can be calculated using the `tf.reduce_mean(tensor)` function. \n","\n","Once we have defined a loss, tensorflow gives us convenient tools to minimise it. Arguably, the simplest (but not necessarily the best) method to minimise any cost function is gradient descent, in which in each step the gradient of the cost function w.r.t to the parameters is calculated, multiplied with a learning rate, and then subtracted from current parameter values. Here, we use gradient descent with momentum, which is similar but also takes past gradient updates into account. Good values for the learning rate and to a lesser degree the momentum term, are hugely important and can make all the difference for successful training."]},{"cell_type":"code","metadata":{"collapsed":true,"id":"N1owYyee5tu4"},"source":["### IMPLEMENT THIS ###\n","# loss_pl = ...\n","######################\n","\n","optimiser = tf.train.AdamOptimizer(learning_rate=0.001)\n","training_op = optimiser.minimize(loss_pl)  # each time this is called a gradient update is performed\n","\n","# Prepare input images again \n","F = np.expand_dims(img, axis=-1)  # add the missing colour channel\n","S_gt = sobel_eval # the ground-truth Sobel filter output evaluated above"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"7nFnybpy5tu4"},"source":["Now that we have a training operation we can run a loop over several iterations to minimise the cost function. In the example below insert the training operation and in the same `run` call also retrieve the current loss. \n","\n","In order to also keep visual track of the optimisation, let's evaluate and plot the filtered image every 500 steps. "]},{"cell_type":"code","metadata":{"collapsed":true,"id":"DHYy0j7t5tu4"},"source":["epsilon = 0.0000001\n","\n","init_op = tf.global_variables_initializer()  # since we now have variables again we need an init operation\n","\n","sess = tf.Session(config=config)\n","\n","sess.run(init_op)\n","\n","for iter in range(10000):\n","\n","    loss, _ = sess.run([loss_pl, training_op], feed_dict={img_pl: F + epsilon, target_pl: S_gt})  # implement this\n","    \n","    if iter % 100 == 0:\n","        print(\"iter: %d, loss: %f\" %(iter, loss))\n","\n","    if iter % 500 == 0:\n","        sobel_approx = sess.run(sobel_pl, feed_dict={img_pl: F})\n","        plt.figure(figsize=(8,6))\n","        plt.subplot(1,3,1)\n","        plt.imshow(np.squeeze(S_gt), cmap='gray')\n","        plt.title('Target img')\n","        plt.subplot(1,3,2)\n","        plt.imshow(np.squeeze(sobel_approx), cmap='gray')\n","        plt.title('Estimated img')\n","        plt.subplot(1,3,3)\n","        plt.imshow(np.squeeze(sobel_approx)-np.squeeze(S_gt), cmap='gray')\n","        plt.title('Difference')\n","        plt.show()\n","\n","    if loss < 0.001:\n","        print(\"Loss below threshold. Exiting optimisation\")\n","        break"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"DQxZhwR45tu5"},"source":["You see that this optimisation very quickly learns to produce a reasonably close approximation to the Sobel filter. Does that mean that it learned exactly the same filters as the Sobel filter? Think about this for a second. Is the solution unique? \n","\n","Now let us have a look at the original filters and at the learned filters. In order to get the actual values of the filter variables we need to evaluated them in our session. "]},{"cell_type":"code","metadata":{"collapsed":true,"id":"jdNgO2OG5tu5"},"source":["# Evaluating the value of the learned convolutional filters\n","H_x_approx = sess.run(h_x_var) \n","H_y_approx = sess.run(h_y_var) \n","\n","# Displaying original and new filters\n","print('The original gradient operations in x and y direction looked like this:')\n","\n","plt.subplot(121)\n","plt.imshow(np.squeeze(H_x), cmap='gray')\n","plt.subplot(122)\n","plt.imshow(np.squeeze(H_y), cmap='gray')\n","plt.show()\n","\n","print('The newly learned filters look like this')\n","plt.subplot(121)\n","plt.imshow(np.squeeze(H_x_approx), cmap='gray')\n","plt.subplot(122)\n","plt.imshow(np.squeeze(H_y_approx), cmap='gray')\n","plt.show()"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"collapsed":true,"id":"zDlwFK1i5tu5"},"source":["You will notice that even though the output of the operation looks almost exactly the same the learned filters are most likely different from the original ones. This is due to the fact that taking the gradient in X and Y directions is an arbitrary definition set by us. We may as well take the the gradient in the directions rotated by 45 degrees or any other angle.\n","\n","Even though this was a relatively simple example, training convolutional neural networks follows almost exactly the same principles. The main difference is that the output of one of the convolutional layers serves as the input to the next convolutional layer (after being modified by a non-linearity function). \n","\n","Similar, to this example convolutional kernels in for machine learning tasks will converge to whatever is \"best\" for the task. However, there is no unique solution. "]},{"cell_type":"code","metadata":{"collapsed":true,"id":"1MzduOd25tu5"},"source":["sess.close()"],"execution_count":null,"outputs":[]}]}