{"nbformat":4,"nbformat_minor":0,"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.6.6"},"colab":{"name":"01d_High-dimensional_data-Predicting_Alzheimers_Disease_and_Age_Regression.ipynb","provenance":[]}},"cells":[{"cell_type":"markdown","metadata":{"id":"DjXYfVhL1XzR"},"source":["***\n","<h2> <u>Goals of this notebook</u> </h2>\n","\n","* So far, we looked at linear models and non-linear models for regression and classification on toy data.\n","* In this notebook, we consider **real-world datasets**!\n","\n","*** \n","<h2> <u>What am I supposed to do?</u> </h2>\n","\n","* **The code in the all the cells in this notebook is already written!**\n","* So sit back and relax! Simply go through the notebook, execute the cells and try to understand what is going on. Feel free to insert new code cells in between and print stuff in order to better understand what is going on.\n","\n","***\n","***\n","<h2> <u>Import required modules</u> </h2>\n"]},{"cell_type":"code","metadata":{"id":"w26hixKn1XzW"},"source":["import numpy as np\n","import matplotlib.pylab as plt"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"0Th4rxGN1a4f"},"source":["<h2> <u>Mount Google drive folder</u> </h2>"]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"XGYaoqrW1bFq","executionInfo":{"status":"ok","timestamp":1615922191512,"user_tz":-60,"elapsed":21675,"user":{"displayName":"Ertunç Erdil","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GhMsoSHG0vnIOh_eNGTGwbaeoCOl-6inFuL6AA4=s64","userId":"03311864043264945628"}},"outputId":"fb2c46c6-ae6c-4fc3-fb32-182f91b10dbc"},"source":["# Mount Google drive\n","from google.colab import drive\n","drive.mount('/content/drive')"],"execution_count":null,"outputs":[{"output_type":"stream","text":["Mounted at /content/drive\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"9JQHpEHY1i0q","executionInfo":{"status":"ok","timestamp":1615922193149,"user_tz":-60,"elapsed":589,"user":{"displayName":"Ertunç Erdil","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GhMsoSHG0vnIOh_eNGTGwbaeoCOl-6inFuL6AA4=s64","userId":"03311864043264945628"}},"outputId":"bdc43df0-19f4-4dea-8c39-d9df832a8a1d"},"source":["cd /content/drive/My Drive/ML_workshop"],"execution_count":null,"outputs":[{"output_type":"stream","text":["/content/drive/My Drive/ML_workshop\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"57fyodcA1ltq","executionInfo":{"status":"ok","timestamp":1615922194179,"user_tz":-60,"elapsed":522,"user":{"displayName":"Ertunç Erdil","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GhMsoSHG0vnIOh_eNGTGwbaeoCOl-6inFuL6AA4=s64","userId":"03311864043264945628"}},"outputId":"c8001cc7-40f2-47e6-9591-8d0a45f5b7aa"},"source":["ls"],"execution_count":null,"outputs":[{"output_type":"stream","text":["\u001b[0m\u001b[01;34mdeep_learning\u001b[0m/  \u001b[01;34mmachine_learning\u001b[0m/\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"uH8zlNIl1XzX"},"source":["***\n","<h2>High-dimensional data</h2>\n","\n","Imaging data often yields high-dimensional features. Most of the time the number of dimensions is much higher than the number of samples. In these cases, a few difficulties arise: \n","- it is difficult to visualize classification or regresion models\n","- when the number of samples is lower than number of dimensions, this may lead to trivial solutions\n","- models need to use some sort of feature selection or regularization to overcome this challenge\n","\n","In this section, we will focus on two high-dimensional problems in neuroimaging. We will use measurements extracted from brain MRI: volumes of different anatomical structures and thickness of the cortical mantle at different locations (all shown in the images below).\n","\n","<img src=\"brain.png\" alt=\"Brain\"  style=\"float: left; width: 15%; margin-right: 1%; margin-bottom: 0.5em;\">\n","\n","<img src=\"aseg.png\" alt=\"ASEG\" style=\"float: left; width: 15%; margin-right: 1%; margin-bottom: 0.5em;\">\n","\n","<img src=\"cort_thickness.png\" alt=\"CORT\" style=\"float: left; width: 15%; margin-right: 1%; margin-bottom: 0.5em;\">\n","<p style=\"clear: both;\">\n","\n","***\n","***\n","<h3>Alzheimer's Disease classification</h3>\n","\n","First one is to classify subjects with Alzheimer's Disease (AD) and healthy elderly (CN) using cortical thickness maps. We will directly work with the cortical thickness values extracted from 290 individuals and aligned on the same reference frame (figure on the right shows one such example)."]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"3h-DhGoU1XzX","executionInfo":{"status":"ok","timestamp":1615922202648,"user_tz":-60,"elapsed":4307,"user":{"displayName":"Ertunç Erdil","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GhMsoSHG0vnIOh_eNGTGwbaeoCOl-6inFuL6AA4=s64","userId":"03311864043264945628"}},"outputId":"b8847fc9-4fea-4fa8-e851-3cc45c1b23c4"},"source":["# features are saved in a matrix\n","features = np.loadtxt('machine_learning/data/features_ad_classification.txt')\n","# labels are saved as a vector\n","labels = np.loadtxt('machine_learning/data/labels_ad_classification.txt')\n","# printing information on the dataset\n","print(\"Number of subjects: {}\".format(features.shape[0]))\n","print(\"Number of features (thickness values): {}\".format(features.shape[1]))\n","print(\"Number of AD cases: {}\".format(np.sum(labels)))\n","print(\"Number of CN cases: {}\".format(np.sum(1-labels)))"],"execution_count":null,"outputs":[{"output_type":"stream","text":["Number of subjects: 290\n","Number of features (thickness values): 10242\n","Number of AD cases: 145.0\n","Number of CN cases: 145.0\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"2urw5bTi1XzX"},"source":["<h3> For this, we use a linear <a href=\"https://en.wikipedia.org/wiki/Support_vector_machine\">support vector machine</a> model.</h3>\n","\n","This model builds a classifier for automatically discriminating subjects with Alzheimer's disease from healthy elderly using the data we just loaded.\n","\n","After training the model, we compute prediction error on the training set (using the entire dataset) and estimate accuracy using a technique known as 'cross validation'. Unfortunately, we do not have the time to discuss the details of this technique. However, you may think of it as a proxy for the test accuracy.\n","\n","We use classification accuracy and the accuracy_score function as in the previous notebook. "]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"SS8Jh8Cf1XzY","executionInfo":{"status":"ok","timestamp":1615922215656,"user_tz":-60,"elapsed":8900,"user":{"displayName":"Ertunç Erdil","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GhMsoSHG0vnIOh_eNGTGwbaeoCOl-6inFuL6AA4=s64","userId":"03311864043264945628"}},"outputId":"75acc318-fa2e-44bf-92bd-c47dbd97967c"},"source":["# import the required function to compute classification accuracy\n","from sklearn.metrics import accuracy_score\n","from sklearn import svm\n","svml = svm.SVC(kernel='linear')\n","\n","# Computing prediction error on the training set. \n","svml.fit(features, labels) # train on all the data\n","preds = svml.predict(features)\n","print(\"Prediction accuracy on the training set: {}\\n\".format(accuracy_score(labels,preds)))\n","\n","# import the required function to perform 5-fold stratified cross-validation\n","# in stratified K-fold cross validation in each fold the ratio of the \n","# number of different classes is the same as the entire dataset. \n","from sklearn.model_selection import StratifiedKFold\n","\n","# creating an object to create partitions for the 5 fold cross validation\n","numFolds = 5\n","skf = StratifiedKFold(n_splits=numFolds)\n","\n","# creating a vector to hold accuracies of different folds: \n","acc_vec = np.zeros(numFolds)\n","# in this for loop we go over different partitions. \n","n = 0\n","for trainind, testind in skf.split(features, labels):\n","    # training both classification models using the training partitions of the dataset. \n","    svml.fit(features[trainind,:], labels[trainind])\n","    \n","    # predictions in the test partition of each fold\n","    preds_cv = svml.predict(features[testind,:])\n","    \n","    # computing accuracy for the test partitions\n","    acc_vec[n] = accuracy_score(labels[testind], preds_cv)\n","    n += 1\n","\n","print(\"Accuracies at different folds:\")\n","print(\"=============================\")\n","print(\"Linear SVM: {}\".format(acc_vec))\n","print(\"\\n\")\n","print(\"Generalization accuracy estimates:\")\n","print(\"=============================\")\n","print(\"Linear SVM: {}\".format(np.mean(acc_vec)))\n"],"execution_count":null,"outputs":[{"output_type":"stream","text":["Prediction accuracy on the training set: 1.0\n","\n","Accuracies at different folds:\n","=============================\n","Linear SVM: [0.86206897 0.93103448 0.9137931  0.75862069 0.84482759]\n","\n","\n","Generalization accuracy estimates:\n","=============================\n","Linear SVM: 0.8620689655172414\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"TjSHfz2h1XzY"},"source":["***\n","***\n","<h3>Age regression</h3>\n","\n","In the second task, we will perform age regression using volumes of different anatomical structures. The underlying idea is that as humans age changes happen in the brain. Certain structures get larger and others get smaller. \n","\n","Let us first read the dataset:"]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"BFu97UEp1XzY","executionInfo":{"status":"ok","timestamp":1615922220618,"user_tz":-60,"elapsed":1623,"user":{"displayName":"Ertunç Erdil","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GhMsoSHG0vnIOh_eNGTGwbaeoCOl-6inFuL6AA4=s64","userId":"03311864043264945628"}},"outputId":"5ad9c712-c289-4fb7-968c-af5d82affb2b"},"source":["# features are saved in a matrix\n","# note that here we read a csv file with np.loadtxt - this is another alternative to reading csv files\n","features = np.loadtxt('machine_learning/data/features_age_regression.csv', delimiter=',').T\n","# labels are saved as a vector\n","labels = np.loadtxt('machine_learning/data/labels_age_regression.csv', delimiter=',')\n","# printing information on the dataset\n","print(\"Number of subjects: {}\".format(features.shape[0]))\n","print(\"Number of features: {}\".format(features.shape[1]))\n","print(\"Mean age in the dataset: {}\".format(np.mean(labels)))\n","print(\"Min / Max age in the dataset: {}/{}\".format(np.min(labels), np.max(labels)))"],"execution_count":null,"outputs":[{"output_type":"stream","text":["Number of subjects: 335\n","Number of features: 45\n","Mean age in the dataset: 43.86865671641791\n","Min / Max age in the dataset: 18.0/94.0\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"nh1yoMOi1XzZ"},"source":["<h3> Here, we use a <a href=\"https://en.wikipedia.org/wiki/Lasso_(statistics)\">LASSO</a> model.</h3>\n","\n","This model builds a regressor for automatically predicting subjects' age from the volumes of anatomical structures, which we have read from file in the previous step. \n","\n","After training the model, we compute prediction error on the training set (using the entire dataset) and estimate accuracy using a technique known as 'cross validation'. Unfortunately, we do not have the time to discuss the details of this technique. However, you may think of it as a proxy for the test accuracy.\n","\n","We use RMSE to compute the prediction error."]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"8DfgOeYQ1XzZ","executionInfo":{"status":"ok","timestamp":1615922225181,"user_tz":-60,"elapsed":484,"user":{"displayName":"Ertunç Erdil","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GhMsoSHG0vnIOh_eNGTGwbaeoCOl-6inFuL6AA4=s64","userId":"03311864043264945628"}},"outputId":"195f1cb9-2632-460c-82c5-f8e0608b5e2e"},"source":["# import the required function to compute classification accuracy\n","from sklearn.metrics import mean_squared_error\n","from sklearn import linear_model\n","lasso = linear_model.Lasso()\n","\n","# Computing prediction error on the training set. \n","lasso.fit(features, labels) # train on all the data\n","preds = lasso.predict(features)\n","print(\"RMSE on the training set: {}\\n\".format(np.sqrt(mean_squared_error(labels,preds))))\n","print(\"Pearson's correlation coefficient on the training set: {}\\n\".format(np.corrcoef(labels,preds)[0,1]))\n","\n","# import the required function to perform 5-fold stratified cross-validation\n","# in stratified K-fold cross validation in each fold the ratio of the \n","# number of different classes is the same as the entire dataset. \n","from sklearn.model_selection import KFold\n","\n","# creating an object to create partitions for the 5 fold cross validation\n","numFolds = 5\n","skf = KFold(n_splits=numFolds)\n","\n","# creating a vector to hold accuracies of different folds: \n","rmse_vec = np.zeros(numFolds)\n","r_vec = np.zeros(numFolds)\n","# in this for loop we go over different partitions. \n","n = 0\n","for trainind, testind in skf.split(features, labels):\n","    # training both classification models using the training partitions of the dataset. \n","    lasso.fit(features[trainind,:], labels[trainind])\n","    \n","    # predictions in the test partition of each fold\n","    preds_cv = lasso.predict(features[testind,:])\n","    \n","    # computing accuracy for the test partitions\n","    rmse_vec[n] = np.sqrt(mean_squared_error(labels[testind], preds_cv))\n","    r_vec[n] = np.corrcoef(labels[testind],preds_cv)[0,1]\n","                          \n","    n += 1\n","\n","print(\"Accuracies at different folds:\")\n","print(\"=============================\")\n","print(\"LASSO - RMSE: {}\".format(rmse_vec))\n","print(\"LASSO - r: {}\".format(r_vec))\n","print(\"\\n\")\n","print(\"Generalization accuracy estimates:\")\n","print(\"=============================\")\n","print(\"LASSO - RMSE: {}\".format(np.mean(rmse_vec)))\n","print(\"LASSO - r: {}\".format(np.mean(r_vec)))"],"execution_count":null,"outputs":[{"output_type":"stream","text":["RMSE on the training set: 13.274767541908702\n","\n","Pearson's correlation coefficient on the training set: 0.8297023861280493\n","\n","Accuracies at different folds:\n","=============================\n","LASSO - RMSE: [12.45859544 13.56866578 14.68075349 15.8606269  11.86957188]\n","LASSO - r: [0.83871396 0.82190906 0.82123691 0.75776068 0.84400263]\n","\n","\n","Generalization accuracy estimates:\n","=============================\n","LASSO - RMSE: 13.687642695718148\n","LASSO - r: 0.8167246479485117\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"cCtCcUYk1Xza"},"source":[""],"execution_count":null,"outputs":[]}]}