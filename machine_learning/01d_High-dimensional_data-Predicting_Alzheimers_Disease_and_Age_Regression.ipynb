{"nbformat":4,"nbformat_minor":0,"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.6.6"},"colab":{"name":"01d_High-dimensional_data-Predicting_Alzheimers_Disease_and_Age_Regression.ipynb","provenance":[]}},"cells":[{"cell_type":"markdown","metadata":{"id":"DjXYfVhL1XzR"},"source":["***\n","<h2> <u>Goals of this notebook</u> </h2>\n","\n","* So far, we looked at linear models and non-linear models for regression and classification on toy data.\n","* In this notebook, we consider **real-world datasets**!\n","\n","*** \n","<h2> <u>What am I supposed to do?</u> </h2>\n","\n","* **The code in the all the cells in this notebook is already written!**\n","* So sit back and relax! Simply go through the notebook, execute the cells and try to understand what is going on. Feel free to insert new code cells in between and print stuff in order to better understand what is going on.\n","\n","***\n","***\n","<h2> <u>Import required modules</u> </h2>\n"]},{"cell_type":"code","metadata":{"id":"w26hixKn1XzW"},"source":["import numpy as np\n","import matplotlib.pylab as plt"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"0Th4rxGN1a4f"},"source":["<h2> <u>Mount Google drive folder</u> </h2>"]},{"cell_type":"code","metadata":{"id":"XGYaoqrW1bFq"},"source":["# Mount Google drive\n","from google.colab import drive\n","drive.mount('/content/drive')"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"9JQHpEHY1i0q"},"source":["cd /content/drive/My Drive/ML_workshop"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"57fyodcA1ltq"},"source":["ls"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"uH8zlNIl1XzX"},"source":["***\n","<h2>High-dimensional data</h2>\n","\n","Imaging data often yields high-dimensional features. Most of the time the number of dimensions is much higher than the number of samples. In these cases, a few difficulties arise: \n","- it is difficult to visualize classification or regresion models\n","- when the number of samples is lower than number of dimensions, this may lead to trivial solutions\n","- models need to use some sort of feature selection or regularization to overcome this challenge\n","\n","In this section, we will focus on two high-dimensional problems in neuroimaging. We will use measurements extracted from brain MRI: volumes of different anatomical structures and thickness of the cortical mantle at different locations.\n","***\n","***\n","<h3>Alzheimer's Disease classification</h3>\n","\n","First one is to classify subjects with Alzheimer's Disease (AD) and healthy elderly (CN) using cortical thickness maps. We will directly work with the cortical thickness values extracted from 290 individuals and aligned on the same reference frame."]},{"cell_type":"code","metadata":{"id":"3h-DhGoU1XzX"},"source":["# features are saved in a matrix\n","features = np.loadtxt('machine_learning/data/features_ad_classification.txt')\n","# labels are saved as a vector\n","labels = np.loadtxt('machine_learning/data/labels_ad_classification.txt')\n","# printing information on the dataset\n","print(\"Number of subjects: {}\".format(features.shape[0]))\n","print(\"Number of features (thickness values): {}\".format(features.shape[1]))\n","print(\"Number of AD cases: {}\".format(np.sum(labels)))\n","print(\"Number of CN cases: {}\".format(np.sum(1-labels)))"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"2urw5bTi1XzX"},"source":["<h3> For this, we use a linear <a href=\"https://en.wikipedia.org/wiki/Support_vector_machine\">support vector machine</a> model.</h3>\n","\n","This model builds a classifier for automatically discriminating subjects with Alzheimer's disease from healthy elderly using the data we just loaded.\n","\n","After training the model, we compute prediction error on the training set (using the entire dataset) and estimate accuracy using a technique known as 'cross validation'.\n","\n","We use classification accuracy and the accuracy_score function as in the previous notebook. "]},{"cell_type":"code","metadata":{"id":"SS8Jh8Cf1XzY"},"source":["# import the required function to compute classification accuracy\n","from sklearn.metrics import accuracy_score\n","from sklearn import svm\n","svml = svm.SVC(kernel='linear')\n","\n","# Computing prediction error on the training set. \n","svml.fit(features, labels) # train on all the data\n","preds = svml.predict(features)\n","print(\"Prediction accuracy on the training set: {}\\n\".format(accuracy_score(labels,preds)))\n","\n","# import the required function to perform 5-fold stratified cross-validation\n","# in stratified K-fold cross validation in each fold the ratio of the \n","# number of different classes is the same as the entire dataset. \n","from sklearn.model_selection import StratifiedKFold\n","\n","# creating an object to create partitions for the 5 fold cross validation\n","numFolds = 5\n","skf = StratifiedKFold(n_splits=numFolds)\n","\n","# creating a vector to hold accuracies of different folds: \n","acc_vec = np.zeros(numFolds)\n","# in this for loop we go over different partitions. \n","n = 0\n","for trainind, testind in skf.split(features, labels):\n","    # training both classification models using the training partitions of the dataset. \n","    svml.fit(features[trainind,:], labels[trainind])\n","    \n","    # predictions in the test partition of each fold\n","    preds_cv = svml.predict(features[testind,:])\n","    \n","    # computing accuracy for the test partitions\n","    acc_vec[n] = accuracy_score(labels[testind], preds_cv)\n","    n += 1\n","\n","print(\"Accuracies at different folds:\")\n","print(\"=============================\")\n","print(\"Linear SVM: {}\".format(acc_vec))\n","print(\"\\n\")\n","print(\"Generalization accuracy estimates:\")\n","print(\"=============================\")\n","print(\"Linear SVM: {}\".format(np.mean(acc_vec)))\n"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"TjSHfz2h1XzY"},"source":["***\n","***\n","<h3>Age regression</h3>\n","\n","In the second task, we will perform age regression using volumes of different anatomical structures. The underlying idea is that as humans age changes happen in the brain. Certain structures get larger and others get smaller. \n","\n","Let us first read the dataset:"]},{"cell_type":"code","metadata":{"id":"BFu97UEp1XzY"},"source":["# features are saved in a matrix\n","# note that here we read a csv file with np.loadtxt - this is another alternative to reading csv files\n","features = np.loadtxt('machine_learning/data/features_age_regression.csv', delimiter=',').T\n","# labels are saved as a vector\n","labels = np.loadtxt('machine_learning/data/labels_age_regression.csv', delimiter=',')\n","# printing information on the dataset\n","print(\"Number of subjects: {}\".format(features.shape[0]))\n","print(\"Number of features: {}\".format(features.shape[1]))\n","print(\"Mean age in the dataset: {}\".format(np.mean(labels)))\n","print(\"Min / Max age in the dataset: {}/{}\".format(np.min(labels), np.max(labels)))"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"nh1yoMOi1XzZ"},"source":["<h3> Here, we use a <a href=\"https://en.wikipedia.org/wiki/Lasso_(statistics)\">LASSO</a> model.</h3>\n","\n","This model builds a regressor for automatically predicting subjects' age from the volumes of anatomical structures, which we have read from file in the previous step. \n","\n","After training the model, we compute prediction error on the training set (using the entire dataset) and estimate accuracy using a technique known as 'cross validation'.\n","\n","We use RMSE to compute the prediction error."]},{"cell_type":"code","metadata":{"id":"8DfgOeYQ1XzZ"},"source":["# import the required function to compute classification accuracy\n","from sklearn.metrics import mean_squared_error\n","from sklearn import linear_model\n","lasso = linear_model.Lasso()\n","\n","# Computing prediction error on the training set. \n","lasso.fit(features, labels) # train on all the data\n","preds = lasso.predict(features)\n","print(\"RMSE on the training set: {}\\n\".format(np.sqrt(mean_squared_error(labels,preds))))\n","print(\"Pearson's correlation coefficient on the training set: {}\\n\".format(np.corrcoef(labels,preds)[0,1]))\n","\n","# import the required function to perform 5-fold stratified cross-validation\n","# in stratified K-fold cross validation in each fold the ratio of the \n","# number of different classes is the same as the entire dataset. \n","from sklearn.model_selection import KFold\n","\n","# creating an object to create partitions for the 5 fold cross validation\n","numFolds = 5\n","skf = KFold(n_splits=numFolds)\n","\n","# creating a vector to hold accuracies of different folds: \n","rmse_vec = np.zeros(numFolds)\n","r_vec = np.zeros(numFolds)\n","# in this for loop we go over different partitions. \n","n = 0\n","for trainind, testind in skf.split(features, labels):\n","    # training both classification models using the training partitions of the dataset. \n","    lasso.fit(features[trainind,:], labels[trainind])\n","    \n","    # predictions in the test partition of each fold\n","    preds_cv = lasso.predict(features[testind,:])\n","    \n","    # computing accuracy for the test partitions\n","    rmse_vec[n] = np.sqrt(mean_squared_error(labels[testind], preds_cv))\n","    r_vec[n] = np.corrcoef(labels[testind],preds_cv)[0,1]\n","                          \n","    n += 1\n","\n","print(\"Accuracies at different folds:\")\n","print(\"=============================\")\n","print(\"LASSO - RMSE: {}\".format(rmse_vec))\n","print(\"LASSO - r: {}\".format(r_vec))\n","print(\"\\n\")\n","print(\"Generalization accuracy estimates:\")\n","print(\"=============================\")\n","print(\"LASSO - RMSE: {}\".format(np.mean(rmse_vec)))\n","print(\"LASSO - r: {}\".format(np.mean(r_vec)))"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"cCtCcUYk1Xza"},"source":[""],"execution_count":null,"outputs":[]}]}