{"nbformat":4,"nbformat_minor":0,"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.6.6"},"colab":{"name":"01c_Using_different_methods-Binary_classification_example.ipynb","provenance":[]}},"cells":[{"cell_type":"markdown","metadata":{"id":"gL39964zyfgW"},"source":["***\n","<h2> <u>Goals of this notebook</u> </h2>\n","\n","* So far, we looked at linear models for regression and classification.\n","* In this notebook, we consider **more complicated models**!\n","\n","*** \n","<h2> <u>What am I supposed to do?</u> </h2>\n","\n","* **The code in the all the cells in this notebook is already written!**\n","* So sit back and relax! Simply go through the notebook, execute the cells and try to understand what is going on. Feel free to insert new code cells in between and print stuff in order to better understand what is going on.\n","\n","***\n","***\n","<h2> <u>Import required modules</u> </h2>\n"]},{"cell_type":"code","metadata":{"id":"isYT51W6yfgd"},"source":["import numpy as np\n","import matplotlib.pylab as plt\n","import pandas as pd"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"HXbYn9jqyqdS"},"source":["<h2> <u>Mount Google drive folder</u> </h2>\n","\n","\n","\n"]},{"cell_type":"code","metadata":{"id":"rwRwR5Uvyq-q"},"source":["# Mount Google drive\n","from google.colab import drive\n","drive.mount('/content/drive')"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"JgbfZCkQy3ri"},"source":["cd /content/drive/My Drive/ML_workshop"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"PCnXN8uDy47s"},"source":["ls"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"xL9Tetfzyfge"},"source":["***\n","Let us first define some helper functions.\n","\n","Function for data visualization: "]},{"cell_type":"code","metadata":{"id":"N__n9Ms_yfgf"},"source":["def VisualizeBinaryClassificationData(features, labels):\n","    # function to visualize binary classification data. \n","    # function only visualizes the first two features in 2D cartesian grid. \n","    # Samples with label = 1 are depicted with \"+\" and those with label = 0 with \"-\"\n","    plt.figure()\n","    pos_rows = labels > 0\n","    neg_rows = labels <= 0\n","    plt.plot(features[pos_rows,0],features[pos_rows,1],'+',markersize=10,mew=2)\n","    plt.plot(features[neg_rows,0],features[neg_rows,1],'_',markersize=10,mew=2)\n","    plt.grid('on')\n","    plt.xlabel('feature1',fontsize=16), plt.ylabel('feature2',fontsize=16)\n","    plt.show()"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"HZ6R8oG4yfgf"},"source":["***\n","Function for visualization of decision boundaries: "]},{"cell_type":"code","metadata":{"id":"w0qKJgQ8yfgf"},"source":["def VisBinClassDecisionBoundaries(model, xlims=[-30,30], ylims=[-30,30], h=0.05, features=None, \n","                                  labels=None, alpha=0.25, title=None):\n","    # function visualizes decision boundaries using color plots\n","    # model is the classification model that can be any model in the scikit-learn package\n","    \n","    # creating meshgrid for different values of features\n","    xx, yy = np.meshgrid(np.arange(xlims[0], xlims[1], h), np.arange(ylims[0], ylims[1], h))\n","    # extracting predictions at different points in the mesh\n","    Z = model.predict(np.c_[xx.ravel(), yy.ravel()])\n","    Z = Z.reshape(xx.shape)\n","    # plotting the mesh\n","    plt.figure()\n","    plt.pcolormesh(xx, yy, Z, cmap=plt.cm.Paired, alpha=alpha)\n","    plt.grid()\n","    \n","    # if the samples are given plot them on the same plot\n","    if (features is not None) and (labels is not None): \n","        pos_rows = labels > 0\n","        neg_rows = labels <= 0\n","        plt.plot(features[pos_rows,0],features[pos_rows,1],'k+',markersize=10,mew=2)\n","        plt.plot(features[neg_rows,0],features[neg_rows,1],'r_',markersize=10,mew=2)\n","        plt.grid('on')\n","        \n","    plt.xlabel('feature1',fontsize=16), plt.ylabel('feature2',fontsize=16)\n","    if title is not None: \n","        plt.title(title, fontsize=16)\n","    plt.show() "],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"3n_AD04Eyfgg"},"source":["***\n","* So far, we have only seen regression of continuous labels with linear models and binary classification with logistic regression.\n","\n","* In this section, we will first see different models for the binary classification problem. \n","\n","* Let us start once more by reading training data and visualizing the samples. \n","\n","* **Note:** For the problem we have been working on so far, the feature dimension is only two. This allows for easy visualization of the samples and the class separation. In more complicated problems with higher number of feature dimensions, such visualization may not be feasible. In these cases, we might have use basic one dimensional histograms and visualize data in a univariate fashion."]},{"cell_type":"code","metadata":{"id":"yB_nLIA3yfgg"},"source":["# Reading data\n","features = pd.read_csv('machine_learning/data/features_linear_classification.csv')\n","labels = pd.read_csv('machine_learning/data/labels_linear_classification.csv')\n","\n","# converting Pandas data frame into numpy arrays: \n","features = features.values\n","labels = labels['0'].values\n","\n","# Plotting\n","VisualizeBinaryClassificationData(features, labels)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"Ygl_38lJyfgg"},"source":["***\n","* In the previous notebook, we have seen how to logistic regression. Let us apply that again for the sake of comparison."]},{"cell_type":"code","metadata":{"id":"XoDB3bB6yfgg"},"source":["# We import the necessary modules to perform logistic regression\n","from sklearn import linear_model\n","\n","# We create an object that can do logistic regression\n","logr = linear_model.LogisticRegression()\n","\n","# We use the data to estimate its parameters with the fit function\n","logr.fit(features, labels)\n","\n","# visualize the decision boundary of the logistic regression model\n","VisBinClassDecisionBoundaries(logr, features=features, labels=labels, title='Logistic Regression')"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"gvh42kCryfgh"},"source":["***\n","* This plot shows the regions where the logistic regression classifier will assign label = 1 and label = 0. \n","* Note that this is a linear model and therefore, the decision boundary is linear.\n","\n","\n","* Now, let us examine other classifier models with different complexities and see the corresponding decision boundaries.\n","* Some of the models are optimized in a stochastic manner. Therefore, they may give slightly different results if they are run multiple times. You can check this behaviour by running each cell two or three times successively.\n","\n","***"]},{"cell_type":"markdown","metadata":{"id":"a4Iam4_hyfgh"},"source":["<h3> Support Vector Machines (SVM) - linear </h3>"]},{"cell_type":"code","metadata":{"id":"QNkxl5FGyfgh"},"source":["from sklearn import svm\n","svml = svm.SVC(kernel='linear')\n","svml.fit(features, labels)\n","VisBinClassDecisionBoundaries(svml, features=features, labels=labels, title='SVM Linear')"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"Q1sSpEiuyfgh"},"source":["<h3> Support Vector Machines (SVM) - polynomial kernel </h3>"]},{"cell_type":"code","metadata":{"id":"snV5a7SFyfgh"},"source":["svmp = svm.SVC(kernel='poly')\n","svmp.fit(features, labels)\n","VisBinClassDecisionBoundaries(svmp, features=features, labels=labels, title='SVM Polynomial')"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"O9DPMFtOyfgi"},"source":["<h3> Decision Trees </h3>"]},{"cell_type":"code","metadata":{"id":"Lk6YI7i3yfgi"},"source":["from sklearn import tree\n","dtree = tree.DecisionTreeClassifier()\n","dtree.fit(features,labels)\n","VisBinClassDecisionBoundaries(dtree, features=features, labels=labels, title='Decision Tree')"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"NC70WRyxyfgi"},"source":["<h3> Random Decision Forests </h3>"]},{"cell_type":"code","metadata":{"id":"-OPXCFHayfgj"},"source":["from sklearn.ensemble import RandomForestClassifier\n","randfor = RandomForestClassifier(n_estimators=50)\n","randfor.fit(features,labels)\n","VisBinClassDecisionBoundaries(randfor, features=features, labels=labels, title='Random Forest')"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"L2ZFVaa1yfgj"},"source":["<h3> Neural Network with 1 hidden layer</h3>"]},{"cell_type":"code","metadata":{"id":"AsUn3BiZyfgj"},"source":["from sklearn.neural_network import MLPClassifier\n","nn1 = MLPClassifier(solver='lbfgs', alpha=1e-5, hidden_layer_sizes=(2), random_state=1)\n","nn1.fit(features, labels)\n","VisBinClassDecisionBoundaries(nn1, features=features, labels=labels, title='Neural Network - 1 hidden layer')"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"oUsJI2Bcyfgj"},"source":["<h3> Neural Network with 5 hidden layers </h3>"]},{"cell_type":"code","metadata":{"id":"N1X89Dj0yfgk"},"source":["nn2 = MLPClassifier(solver='lbfgs', alpha=1e-5, hidden_layer_sizes=(30,30,30,30,30), random_state=1)\n","nn2.fit(features, labels)\n","VisBinClassDecisionBoundaries(nn2, features=features, labels=labels, title='Neural Network - 5 hidden layers')"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"KSOlWzvzyfgk"},"source":["***\n","\n","* In a practical situation, the selection of which model to use is a design choice. \n","* In general, simpler models are less likely to give a very small training error. However, the advantage with simpler models is that they are also less likely to 'overfit' to the training data. Hence, they are more likely to generalize to unseen test data. The general rule of thumb is to follow the spirit of <a href=\"https://en.wikipedia.org/wiki/Occam%27s_razor\">occam's razor</a>: that is, to choose the simplest model that provides satisfactory performance.\n","\n","<h2> Cross validation </h2>\n","\n","Having two or three (in case of validation set) separate datasets may not be feasible for some applications, where available data is limited. In these cases, dividing the already small sample sizes into training and test sets might yield very small training samples and create problems in the estimation of the model parameters. \n","\n","To address small sample sizes, an imperfect but widely used approach is cross-validation. In cross-validation the data is divided into different partitions, for instance $K$. The specific instance of this cross-validation is named K-fold cross-validation. The model is trained and prediction accuracy estimated $K$ times. Each time $K-1$ partitions of the dataset is used for training and the remaining partition is used to compute prediction accuracy. In the next round, another partition is set aside. At the end, each test sample is used once as a test sample. The final generalization accuracy is computed as the average of the individual runs. \n","\n","In scikit-learn this is already implemented. Let us see how it is used. "]},{"cell_type":"code","metadata":{"id":"VbcPogRJyfgk"},"source":["# Reading data\n","training_features = np.loadtxt('machine_learning/data/train_genacc_features.txt')\n","training_labels = np.loadtxt('machine_learning/data/train_genacc_labels.txt')\n","\n","# Plotting\n","VisualizeBinaryClassificationData(training_features, training_labels)\n"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"Miore-1Lyfgk"},"source":["# We import the necessary modules to perform logistic regression\n","from sklearn import linear_model\n","# We create an object that can do logistic regression\n","logr = linear_model.LogisticRegression()\n","# We use the data to estimate its parameters with the fit function\n","logr.fit(training_features, training_labels)\n","\n","# Import the necessary models for MLP classifier\n","from sklearn.neural_network import MLPClassifier\n","# Create an object that will do the classification\n","nn2 = MLPClassifier(solver='lbfgs', alpha=1e-5, hidden_layer_sizes=(3,3), random_state=1)\n","# We use the training data to estimate the parameters. \n","nn2.fit(training_features, training_labels)\n"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"mt8agEY1yfgl"},"source":["# import the required function to perform 5-fold stratified cross-validation\n","# in stratified K-fold cross validation in each fold the ratio of the \n","# number of different classes is the same as the entire dataset. \n","from sklearn.model_selection import StratifiedKFold\n","# creating an object to create partitions for the 5 fold cross validation\n","skf = StratifiedKFold(n_splits=5)\n","\n","# in this for loop we go over different partitions. \n","# \"skf\" object produces indices of samples for different folds\n","for trainind, testind in skf.split(training_features, training_labels):\n","    print(\"Training indices: {}\".format(trainind))\n","    print(\"Test indices: {}\".format(testind))\n"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"hyLWVbH1yfgl"},"source":["Let us now run the entire cross-validation experiment to estimate generalization accuracy and compare it with the training accuracy we computed above:"]},{"cell_type":"code","metadata":{"id":"y3BtuB5Yyfgl"},"source":["# import the required function to perform 5-fold stratified cross-validation\n","# in stratified K-fold cross validation in each fold the ratio of the \n","# number of different classes is the same as the entire dataset. \n","from sklearn.model_selection import StratifiedKFold\n","\n","# import the required function to compute classification accuracy\n","from sklearn.metrics import accuracy_score\n","\n","# creating an object to create partitions for the 5 fold cross validation\n","numFolds = 5\n","skf = StratifiedKFold(n_splits=numFolds)\n","\n","# creating a vector to hold accuracies of different folds: \n","acc_vec_logr = np.zeros(numFolds)\n","acc_vec_nn2  = np.zeros(numFolds)\n","# in this for loop we go over different partitions. \n","n = 0\n","for trainind, testind in skf.split(training_features, training_labels):\n","    # training both classification models using the training partitions of the dataset. \n","    logr.fit(training_features[trainind,:], training_labels[trainind])\n","    nn2.fit(training_features[trainind,:], training_labels[trainind])\n","    \n","    # predictions in the test partition of each fold\n","    preds_cv_logr = logr.predict(training_features[testind,:])\n","    preds_cv_nn2 = nn2.predict(training_features[testind,:])\n","    \n","    # computing accuracy for the test partitions\n","    acc_vec_logr[n] = accuracy_score(training_labels[testind], preds_cv_logr)\n","    acc_vec_nn2[n]  = accuracy_score(training_labels[testind], preds_cv_nn2)\n","    n += 1\n","\n","print(\"Accuracies at different folds:\")\n","print(\"=============================\")\n","print(\"Logistic Regression: {}\".format(acc_vec_logr))\n","print(\"Neural Networks with 2 HL: {}\".format(acc_vec_nn2))\n","print(\"\\n\")\n","print(\"Generalization accuracy estimates:\")\n","print(\"=============================\")\n","print(\"Logistic Regression: {}\".format(np.mean(acc_vec_logr)))\n","print(\"Neural Networks with 2 HL: {}\".format(np.mean(acc_vec_nn2)))"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"0Y6yg6hpyfgm"},"source":["<b>Note:</b> Comparing generalization accuracy estimate computed with 5-fold cross-validation and training set accuracy shows the difference between these two approaches. 5-fold cross-validation is also not a perfect estimation technique however, this time it gets a much closer estimate to the accuracy on the separate test set than prediction accuracy on the training set. \n","\n","<b>Note:</b> Cross-validation is an experiment to estimate generalization accuracy. It is strategy to best make use of the available data. When creating the final model the learning algorithm is trained with all available training data and shipped. "]},{"cell_type":"code","metadata":{"id":"6U9xaWMzyfgm"},"source":[""],"execution_count":null,"outputs":[]}]}