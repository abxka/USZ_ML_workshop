{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pylab as plt\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Even though it is possible to write machine learning models from scratch, this can be a tedious task.\n",
    "\n",
    "Fortunately, there are many different machine learning libraries available in various languages. In this workshop we adopt python and scikit-learn library, which is widely used both in academic research as well as industry. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2> Linear Regression </h2>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The first machine learning algorithm we will use is linear regression. Linear regression has a simple model that can be written as:\n",
    "\n",
    "$y = ax + b$, where $y$ represents labels -- in this case continuous -- and $x$ represent features. \n",
    "\n",
    "The model has two parameters: $a$ and $b$. Our goal, which is one of the main goals in most machine learning algorithms, is to estimate these parameters using the data such that the model is able to predict labels from features as accurate as possible. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reading and visualizing the data\n",
    "features = np.loadtxt('data/features_linear_regression.txt')[:,np.newaxis]\n",
    "labels = np.loadtxt('data/labels_linear_regression.txt')\n",
    "nsamples = features.size\n",
    "print ('Number of samples: {}'.format(nsamples))\n",
    "# Plotting\n",
    "plt.scatter(features, labels, color='b')\n",
    "plt.grid('on')\n",
    "plt.xlabel('features')\n",
    "plt.ylabel('labels')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# we first import the necessary library: \n",
    "from sklearn import linear_model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In scikit-learn machine learning tools are designed as objects. Different algorithms are created and trained using similar syntax. Here we first create a linear regression object and then train it with the data. \n",
    "\n",
    "To train the model we will use the data we just read from the txt files. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We create an object that can do linear regression\n",
    "regr = linear_model.LinearRegression()\n",
    "# We use the data to estimate its parameters with the fit function\n",
    "regr.fit(features, labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The fit function internally solved the problem: \n",
    "\n",
    "$\\arg_{a,b}\\min \\sum_{n=1} ( y_n - (ax_n+b) )^2$\n",
    "\n",
    "We can now look at the linear model it found by simply plotting the line with the determined parameters. The learned parameters are saved in the linear regression object we created \"regr\": "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plotting\n",
    "plt.scatter(features, labels,color='b')\n",
    "plt.grid('on')\n",
    "plt.xlabel('features')\n",
    "plt.ylabel('labels')\n",
    "x = np.asarray([[-0], [30]])\n",
    "plt.plot(x, regr.predict(x), 'r', linewidth=2.5)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "Prediction for unseen data is performed using the object \"regr\" as well. Actually, while plotting line we already used this function \"regr.predict\". \n",
    "\n",
    "Let us read some unseen data from file, predict the labels and plot them. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_features = np.loadtxt('data/test_features_linear_regression.txt')[:,np.newaxis]\n",
    "print(\"Test sample's features:\\n {}\".format(test_features))\n",
    "# We use the predict function of the object to predict for a new set of samples\n",
    "test_predict = regr.predict(test_features)\n",
    "print(\"Predicted labels:\\n {}\".format(test_predict))\n",
    "\n",
    "# Plotting\n",
    "plt.scatter(test_features, test_predict, color='g', s=100)\n",
    "plt.grid('on')\n",
    "plt.xlabel('features')\n",
    "plt.ylabel('labels')\n",
    "x = np.asarray([[-0], [30]])\n",
    "plt.plot(x, regr.predict(x), 'r', linewidth=2.5)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Models are often not perfect. We have also seen in the training data that there is a discrepancy between model line and labels of the data. Such a discrepancy will also exist in the training set. \n",
    "\n",
    "Let us now read the \"true\" labels of the test set and visualize the difference with the model predictions. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_labels = np.loadtxt('data/test_labels_linear_regression.txt')\n",
    "\n",
    "# Plotting\n",
    "plt.scatter(test_features, test_predict, color='g', s=100)\n",
    "plt.scatter(test_features, test_labels, color='k', s=100)\n",
    "plt.grid('on')\n",
    "plt.xlabel('features')\n",
    "plt.ylabel('labels')\n",
    "x = np.asarray([[-0], [30]])\n",
    "plt.plot(x, regr.predict(x), 'r', linewidth=2.5)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can also quantify the discrepancy between model prediction and \"true\" labels using the same cost function as we used in the training part: \n",
    "\n",
    "$\\sum_{n=1} ( y_n - (ax_n+b) )^2$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "total_test_error = np.sum((test_labels - test_predict)**2)\n",
    "mean_squared_error = np.mean((test_labels - test_predict)**2)\n",
    "root_mean_squared_error = np.sqrt(np.mean((test_labels - test_predict)**2))\n",
    "print(\"Total test error: {}\".format(total_test_error))\n",
    "print(\"Mean squared error: {}\".format(mean_squared_error))\n",
    "print(\"Root mean squared error (RMSE): {}\".format(root_mean_squared_error))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2> Exercise 2:</h2>\n",
    "In this small exercise, you will\n",
    "- Read features and labels from txt files and visualize: \n",
    "The feature file : \"data/ex2_features_regression.txt\".\n",
    "The label file : \"data/ex2_labels_regression.txt\"\n",
    "\n",
    "- Fit a linear regression model to the data. Please name the linear regression object differently, e.g. regr_ex\n",
    "\n",
    "- Visualize the learned model \n",
    "\n",
    "- Read features of test samples from another txt file and predict labels: \n",
    "The feature file : \"data/ex2_test_features_regression.txt\"\n",
    "\n",
    "- Read \"true\" labels of the test samples from the last file (data/ex2_test_labels_regression.txt), compare the predicted values with real labels and compute RMSE. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# TODO\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2> Binary Classification with Logistic Regression </h2>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The second task we will focus on is binary classification and we will use Logistic Regression for this task. \n",
    "Logistic regression also has a very simple model:\n",
    "\n",
    "$y = \\sigma(ax + b)$, where $y$ represents labels -- in this case binary -- $x$ represent features and $\\sigma(\\cdot)$ represents the sigmoid function\n",
    "\n",
    "$\\sigma(w) = \\frac{1}{1 + e^{-w}}$\n",
    "\n",
    "The predictions are considered as probabilities, i.e. $p(y=1|x) = \\sigma(ax+b)$\n",
    "\n",
    "In the same formulation we can also consider multiple features, e.g. $x_1$ and $x_2$. In this case the only difference is that the product $ax$ becomes a vector product: $a\\cdot x = a_1x_1 + a_2x_2$. The model becomes: \n",
    "\n",
    "$y = \\sigma(a\\cdot x + b)$\n",
    "\n",
    "Logistic regression also has two parameters $a$ and $b$, and where there are multiple features $a$ will be a vector and composed of multiple parameters.\n",
    "\n",
    "Let us focus on a specific dataset: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reading data\n",
    "features = pd.read_csv('data/features_linear_classification.csv')\n",
    "labels = pd.read_csv('data/labels_linear_classification.csv')\n",
    "\n",
    "# Plotting\n",
    "pos_rows = labels['0'] > 0\n",
    "neg_rows = labels['0'] <= 0\n",
    "plt.plot(features.feature1[pos_rows],features.feature2[pos_rows],'+',markersize=10,mew=2)\n",
    "plt.plot(features.feature1[neg_rows],features.feature2[neg_rows],'_',markersize=10,mew=2)\n",
    "plt.grid('on')\n",
    "plt.xlabel('feature1',fontsize=16), plt.ylabel('feature2',fontsize=16)\n",
    "plt.show()\n",
    "\n",
    "# converting Pandas data frame into numpy arrays: \n",
    "features = features.values\n",
    "labels = labels['0'].values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Logistic regression is in the same module as linear regression in the scikit-learn package. We directly create the necessary object and train the model with the available data: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We create an object that can do logistic regression\n",
    "clas = linear_model.LogisticRegression()\n",
    "# We use the data to estimate its parameters with the fit function\n",
    "clas.fit(features, labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are several things to note here. \n",
    "\n",
    "First, the creation of the logistic regression object and training is done exactly the same way as in linear regression. This extends to almost all models in the scikit-learn package. More importantly, this also extends to almost all machine learning algorithms conceptually. Once you have the data you determine the parameters of the model that best predicts labels from features in the training data. \n",
    "\n",
    "The differences are under the hood. Models differ, which we have seen earlier, and costs functions differ. The main cost function that got minimized for the logistic regression is the cross-entropy: \n",
    "\n",
    "$\\arg_{a,b}\\min \\sum_{n=1} y_n \\ln \\hat{y}_n + (1 - y_n)\\ln (1 - \\hat{y}_n)$, where $\\hat{y}_n=\\sigma(a\\cdot x_n + b)$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can now also visualize the learned model. To do this we will look at the decision boundaries that was created. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plotting\n",
    "pos_rows = labels > 0\n",
    "neg_rows = labels <= 0\n",
    "plt.plot(features[pos_rows,0],features[pos_rows,1],'+',markersize=10,mew=2)\n",
    "plt.plot(features[neg_rows,0],features[neg_rows,1],'_',markersize=10,mew=2)\n",
    "plt.grid('on')\n",
    "plt.xlabel('feature1',fontsize=16), plt.ylabel('feature2',fontsize=16)\n",
    "\n",
    "x = np.asarray([[-20], [25]])\n",
    "# coefficients of the logistic regression are saved in the \"clas\" object and can be constructed into a line as\n",
    "m = clas.coef_[0,0] / clas.coef_[0,1]\n",
    "b = clas.intercept_ / clas.coef_[0,1]\n",
    "plt.plot(x[:,0], b - m*x[:,0], 'r--', linewidth=2)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As before, let us now read features of test samples and perform prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_features = np.loadtxt('data/test_features_linear_classification.txt')\n",
    "print(\"Test sample's features:\\n {}\".format(test_features))\n",
    "# We use the predict function of the object to predict for a new set of samples\n",
    "test_predict = clas.predict(test_features)\n",
    "print(\"Predicted labels:\\n {}\".format(test_predict))\n",
    "\n",
    "# Plotting\n",
    "x = np.asarray([[-20], [25]])\n",
    "m = clas.coef_[0,0] / clas.coef_[0,1]\n",
    "b = clas.intercept_ / clas.coef_[0,1]\n",
    "plt.plot(x[:,0], b - m*x[:,0], 'r--', linewidth=2)\n",
    "plt.grid('on')\n",
    "plt.xlabel('feature1',fontsize=16), plt.ylabel('feature2',fontsize=16)\n",
    "plt.plot(test_features[:,0],test_features[:,1],'p',markersize=10,mew=2)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There is an important point to notice. Prediction is performed in the same way as we have done in the linear regression case. \n",
    "\n",
    "As in the linear regression case -- and as in most cases -- models are not perfect and will make errors when predicting. We can visualize this by looking at the \"true\" labels of the test samples. Let us visualize this first and then quantify the error in terms of \"classification accuracy\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_labels = np.loadtxt('data/test_labels_linear_classification.txt')\n",
    "# Plotting\n",
    "x = np.asarray([[-20], [25]])\n",
    "m = clas.coef_[0,0] / clas.coef_[0,1]\n",
    "b = clas.intercept_ / clas.coef_[0,1]\n",
    "plt.plot(x[:,0], b - m*x[:,0], 'r--', linewidth=2)\n",
    "plt.grid('on')\n",
    "plt.xlabel('feature1',fontsize=16), plt.ylabel('feature2',fontsize=16)\n",
    "# getting the indices of the correct and wrong predictions by comparing with the true labels: \n",
    "correct_predictions = np.where(test_labels == test_predict)[0]\n",
    "wrong_predictions = np.where(test_labels != test_predict)[0]\n",
    "plt.plot(test_features[correct_predictions,0],test_features[correct_predictions,1],'pg',markersize=10,mew=2)\n",
    "plt.plot(test_features[wrong_predictions,0],test_features[wrong_predictions,1],'pr',markersize=10,mew=2)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We see that there are three test samples where logistic regression made the wrong prediction. We can quantify this using different quantities: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clas_accuracy = np.sum(test_predict == test_labels) / test_features.shape[0]\n",
    "clas_fps = np.sum(test_predict > test_labels) / test_features.shape[0]\n",
    "clas_fns = np.sum(test_predict < test_labels) / test_features.shape[0]\n",
    "print('Classification accuracy: {}'.format(clas_accuracy))\n",
    "print('False positive rate: {}'.format(clas_fps))\n",
    "print('False negative rate: {}'.format(clas_fns))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2> Exercise 3:</h2>\n",
    "In this small exercise, you will\n",
    "- Read features and labels from txt files and visualize: \n",
    "The feature file : \"data/ex3_features_classification.txt\".\n",
    "The label file : \"data/ex3_labels_classification.txt\"\n",
    "\n",
    "- Fit a logistic regression model to the data. Please name the classification object differently, e.g. clas_ex\n",
    "\n",
    "- Visualize the boundary - here you can directly copy the form we used above.\n",
    "\n",
    "- Read features of test samples from another txt file and predict labels: \n",
    "The feature file : \"data/ex3_test_features_classification.txt\"\n",
    "\n",
    "- Read \"true\" labels of the test samples from the last file (data/ex3_test_labels_classification.txt), compare the predicted values with real labels and compute classification accuracy and false positive rate. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# TODO\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
