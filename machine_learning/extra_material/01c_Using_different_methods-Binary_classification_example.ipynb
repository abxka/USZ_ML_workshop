{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pylab as plt\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def VisualizeBinaryClassificationData(features, labels):\n",
    "    # function to visualize binary classification data. \n",
    "    # function only visualizes the first two features in 2D cartesian grid. \n",
    "    # Samples with label = 1 are depicted with \"+\" and those with label = 0 with \"-\"\n",
    "    plt.figure()\n",
    "    pos_rows = labels > 0\n",
    "    neg_rows = labels <= 0\n",
    "    plt.plot(features[pos_rows,0],features[pos_rows,1],'+',markersize=10,mew=2)\n",
    "    plt.plot(features[neg_rows,0],features[neg_rows,1],'_',markersize=10,mew=2)\n",
    "    plt.grid('on')\n",
    "    plt.xlabel('feature1',fontsize=16), plt.ylabel('feature2',fontsize=16)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So far, we have only seen regression of continuous labels with linear models and binary classification with logistic regression. In this section, we will first see different models for the binary classification problem. \n",
    "\n",
    "Let us start once more by reading training data and visualizing the samples. \n",
    "\n",
    "<b>Note:</b> For the problem we have been working on so far, the feature dimension is only two. This allows for easy visualization of the samples and the class separation. In more complicated problems with higher number of feature dimensions, such visualization may not be feasible. In these cases we might have use basic one dimensional histograms and visualize data in a univariate fashion."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reading data\n",
    "features = pd.read_csv('data/features_linear_classification.csv')\n",
    "labels = pd.read_csv('data/labels_linear_classification.csv')\n",
    "\n",
    "# converting Pandas data frame into numpy arrays: \n",
    "features = features.values\n",
    "labels = labels['0'].values\n",
    "\n",
    "# Plotting\n",
    "VisualizeBinaryClassificationData(features, labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the previous section we have seen how to logistic regression. Let us apply that again for the sake of comparison. Also, we will see another way to visualize the classification results -- through plotting decision boundaries. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We import the necessary modules to perform logistic regression\n",
    "from sklearn import linear_model\n",
    "# We create an object that can do logistic regression\n",
    "logr = linear_model.LogisticRegression()\n",
    "# We use the data to estimate its parameters with the fit function\n",
    "logr.fit(features, labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def VisBinClassDecisionBoundaries(model, xlims=[-30,30], ylims=[-30,30], h=0.05, features=None, \n",
    "                                  labels=None, alpha=0.25, title=None):\n",
    "    # function visualizes decision boundaries using color plots\n",
    "    # model is the classification model that can be any model in the scikit-learn package\n",
    "    \n",
    "    # creating meshgrid for different values of features\n",
    "    xx, yy = np.meshgrid(np.arange(xlims[0], xlims[1], h), np.arange(ylims[0], ylims[1], h))\n",
    "    # extracting predictions at different points in the mesh\n",
    "    Z = model.predict(np.c_[xx.ravel(), yy.ravel()])\n",
    "    Z = Z.reshape(xx.shape)\n",
    "    # plotting the mesh\n",
    "    plt.figure()\n",
    "    plt.pcolormesh(xx, yy, Z, cmap=plt.cm.Paired, alpha=alpha)\n",
    "    plt.grid()\n",
    "    \n",
    "    # if the samples are given plot them on the same plot\n",
    "    if (features is not None) and (labels is not None): \n",
    "        pos_rows = labels > 0\n",
    "        neg_rows = labels <= 0\n",
    "        plt.plot(features[pos_rows,0],features[pos_rows,1],'k+',markersize=10,mew=2)\n",
    "        plt.plot(features[neg_rows,0],features[neg_rows,1],'r_',markersize=10,mew=2)\n",
    "        plt.grid('on')\n",
    "        \n",
    "    plt.xlabel('feature1',fontsize=16), plt.ylabel('feature2',fontsize=16)\n",
    "    if title is not None: \n",
    "        plt.title(title, fontsize=16)\n",
    "    plt.show() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "VisBinClassDecisionBoundaries(logr, features=features, labels=labels, title='Logistic Regression')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This plot shows the regions where the classifier will assign label = 1 and label = 0. \n",
    "\n",
    "Let us examine different classifier models and see their behavior. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3> Support Vector Machines (SVM) - linear </h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import svm\n",
    "svml = svm.SVC(kernel='linear')\n",
    "svml.fit(features, labels)\n",
    "VisBinClassDecisionBoundaries(svml, features=features, labels=labels, title='SVM Linear')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3> Support Vector Machines (SVM) - polynomial kernel </h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "svmp = svm.SVC(kernel='poly')\n",
    "svmp.fit(features, labels)\n",
    "VisBinClassDecisionBoundaries(svmp, features=features, labels=labels, title='SVM Polynomial')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3> Decision Trees </h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import tree\n",
    "dtree = tree.DecisionTreeClassifier()\n",
    "dtree.fit(features,labels)\n",
    "VisBinClassDecisionBoundaries(dtree, features=features, labels=labels, title='Decision Tree')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3> Random Decision Forests </h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "randfor = RandomForestClassifier(n_estimators=50)\n",
    "randfor.fit(features,labels)\n",
    "VisBinClassDecisionBoundaries(randfor, features=features, labels=labels, title='Random Forest')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3> Neural Networks with 1 hidden layer</h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.neural_network import MLPClassifier\n",
    "nn1 = MLPClassifier(solver='lbfgs', alpha=1e-5, hidden_layer_sizes=(2), random_state=1)\n",
    "nn1.fit(features, labels)\n",
    "VisBinClassDecisionBoundaries(nn1, features=features, labels=labels, title='Neural Network - 1 HL')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3> Neural Networks with 2 hidden layer </h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nn2 = MLPClassifier(solver='lbfgs', alpha=1e-5, hidden_layer_sizes=(3,3), random_state=1)\n",
    "nn2.fit(features, labels)\n",
    "VisBinClassDecisionBoundaries(nn2, features=features, labels=labels, title='Neural Network - 2 HL')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2> Exercise 4:</h2>\n",
    "In this small exercise, you will apply different classification models to the data in \"data/ex4_features_classification.txt\" and \"data/ex4_labels_classification.txt\". The goal is to use the data to train different models, visualize decision boundaries and observe how things change with different parameter settings. Please use the functions we already used above. \n",
    "\n",
    "To this end, choose a model, look-up the description in scikit-learn webpage and choose a set of parameters. Modify the parameters and observe how the classifier's decision boundary changes. \n",
    "\n",
    "As an example, you can choose a random forest model and change the number of estimators (number of trees) and the depth of the trees. These properties can be set with different parameters as explained in the respective website. \n",
    "\n",
    "Another example can be to choose a neural network model and change the number of layers and number of nodes in the hidden layers. \n",
    "\n",
    "<b>Note :</b> The files are txt files. Please read them with np.loadtxt and not pd.read_csv\n",
    "\n",
    "<b>Note :</b> If you choose to work with SVM try the 'rbf' kernel as well."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# TODO\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3> Different models - Regression example </h3> \n",
    "\n",
    "Just as in the classification task, you can also use different methods for the regression task. Let us quickly look at a regression problem and try with decision trees. Specifically, let us consider the example you worked on as the regression exercise earlier today. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reading and visualizing the data\n",
    "features = np.loadtxt('data/ex2_features_regression.txt')[:,np.newaxis]\n",
    "labels = np.loadtxt('data/ex2_labels_regression.txt')\n",
    "\n",
    "# Plotting\n",
    "plt.scatter(features, labels, color='b')\n",
    "plt.grid('on')\n",
    "plt.xlabel('features')\n",
    "plt.ylabel('labels')\n",
    "\n",
    "# We create an object that can do linear regression\n",
    "rtree = tree.DecisionTreeRegressor()\n",
    "# We use the data to estimate its parameters with the fit function\n",
    "rtree.fit(features, labels)\n",
    "\n",
    "# Plotting\n",
    "x = np.arange(0, 30., 0.05)[:,np.newaxis]\n",
    "plt.plot(x, rtree.predict(x), 'r', linewidth=2.5)\n",
    "\n",
    "# Reading features of the test samples and predicting with the learned model: \n",
    "test_features = np.loadtxt('data/ex2_test_features_regression.txt')[:,np.newaxis]\n",
    "print(\"Test sample's features:\\n {}\".format(test_features))\n",
    "test_predict = rtree.predict(test_features)\n",
    "print(\"Predicted labels:\\n {}\".format(test_predict))\n",
    "\n",
    "# Reading true labels and computing RMSE\n",
    "test_labels = np.loadtxt('data/ex2_test_labels_regression.txt')\n",
    "root_mean_squared_error = np.sqrt(np.mean((test_labels - test_predict)**2))\n",
    "print(\"Root mean squared error (RMSE): {}\".format(root_mean_squared_error))\n",
    "\n",
    "# Plotting\n",
    "plt.scatter(test_features, test_predict, color='g', s=100)\n",
    "plt.scatter(test_features, test_labels, color='k', s=100)\n",
    "plt.show() # showing everything on the screen"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that the model captures the non-linearity in the data. It is possibly not the cleanest model as the regressor \"wiggles\" quite a bit. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3> Exercise 5 - optional:</h3>\n",
    "Try out different regression models on the same data. One choice can be random forest regressor."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# TODO\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
