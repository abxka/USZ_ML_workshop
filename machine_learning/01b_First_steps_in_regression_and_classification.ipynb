{"nbformat":4,"nbformat_minor":0,"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.6.6"},"colab":{"name":"01b_First_steps_in_regression_and_classification.ipynb","provenance":[]}},"cells":[{"cell_type":"markdown","metadata":{"id":"qhcFAtEZse7E"},"source":["***\n","<h2> <u>Goals of this notebook</u> </h2>\n","\n","* Now that we understand how to load and visualize data, let's fit some simple machine learning models to data.\n","* Specifically, we will look at one model for regression: '**Linear regression**' and one model for binary classification: '**Logistic regression**'.\n","\n","***\n","<h2> <u>What am I supposed to do?</u> </h2>\n","\n","* As in the first notebook, the code in the first few cells is already written.\n","* Go through the same and understand it. Feel free to insert new code cells in between and print stuff in order to better understand what is going on.\n","* Towards the end, some blocks are left empty for you to fill in.\n","\n","***\n","<h2> <u>Some tips</u> </h2>\n","\n","* The print command is your friend. Once you read anything into a variable v, you can print(v) to see the contents of v. Use this to understand the data inside v. Further, depending on the type of v, you can do print(v.shape), print(len(v)), print(v.size). Use print extensively to understand as well as to debug your own code!"]},{"cell_type":"markdown","metadata":{"id":"7R6SAGw-se7L"},"source":["***\n","***\n","\n","<h2> <u>Import required modules</u> </h2>\n"]},{"cell_type":"code","metadata":{"id":"bpPfxm7Hse7M"},"source":["import numpy as np\n","import matplotlib.pylab as plt\n","import pandas as pd"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"29Z3S9Xcse7N"},"source":["***\n","***\n","\n","Even though it is possible to write machine learning models from scratch, this can be a tedious task. Fortunately, there are many different machine learning libraries available in various languages. In this workshop, we adopt python and scikit-learn library, which is widely used both in academic research as well as in industry.\n","\n","\n","<a href=\"https://scikit-learn.org/stable/modules/classes.html#\">scikit-learn API</a>\n","\n","***\n","***\n","\n","<h2> <u>Linear Regression</u> </h2>\n","\n","* The first machine learning algorithm we will use is linear regression - that is, we will use a linear model to do a regression task.\n","* Linear regression has a simple model that can be written as: $y = ax + b$, where $y$ represents labels (in this case, continuous) and $x$ represent inputs / features.\n","* The model has two parameters: $a$ and $b$. Our goal (which is one of the main goals in most machine learning algorithms) is to estimate these parameters using \"training data\", such that the model is able to predict labels from features of \"test data\" as accurately as possible.\n","\n","***\n","* First, we will read and visualize the data as we did in the 01a notebook."]},{"cell_type":"markdown","metadata":{"id":"i59Cb2--t2Zc"},"source":["<h2> <u>Mount Google drive folder</u> </h2>\n"]},{"cell_type":"code","metadata":{"id":"VLx4Xh8ttCSA"},"source":["# Mount Google drive\n","from google.colab import drive\n","drive.mount('/content/drive')"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"V4AKnut1tOh9"},"source":["cd /content/drive/My Drive/ML_workshop"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"Q08Grrf3t43T"},"source":["ls"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"uxMXclSUse7O"},"source":["# read and visualize the data\n","features = np.loadtxt('machine_learning/data/features_linear_regression.txt')[:,np.newaxis]\n","labels = np.loadtxt('machine_learning/data/labels_linear_regression.txt')\n","nsamples = features.size\n","print ('Number of samples: {}'.format(nsamples))\n","print (features.shape)\n","\n","# plot\n","plt.scatter(features, labels, color='b')\n","plt.grid('on')\n","plt.xlabel('features')\n","plt.ylabel('labels')\n","plt.show()\n"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"nYYisOPCse7O"},"source":["***\n","* Next, we import the necessary module, containing the machine learning models"]},{"cell_type":"code","metadata":{"id":"HHKrIIpbse7O"},"source":["from sklearn import linear_model"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"4HU0Q4wNse7O"},"source":["***\n","* In scikit-learn machine learning tools are designed as objects. Different algorithms are created and trained using similar syntax.\n","\n","* In the following cell, we first create a linear regression object. \n","\n","* Next, we train it using the training data read from the txt files.\n","\n","<a href=\"https://scikit-learn.org/stable/modules/generated/sklearn.linear_model.LinearRegression.html#sklearn.linear_model.LinearRegression\">LinearRegression documentation</a>\n"]},{"cell_type":"code","metadata":{"id":"hzkz7ZONse7P"},"source":["# first, we create an object that can do linear regression\n","regr = linear_model.LinearRegression()\n","\n","# now, we train the model - that is, we use the training data to estimate the model parameters.\n","regr.fit(features, labels)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"rz0LGsXcse7P"},"source":["***\n","* Wow, that was fast!\n","* Internally, the fit function solves the following optimization problem:\n","$\\arg_{a,b}\\min \\sum_{n=1}^{N} ( y_n - (ax_n+b) )^2$\n","\n","***\n","* We can now look at the optimal linear model by plotting the line with the determined parameters.\n","* The learned parameters are saved in the linear regression object we created \"regr\": "]},{"cell_type":"code","metadata":{"id":"O-JOwA9pse7P"},"source":["# plot the trained model\n","plt.scatter(features, labels,color='b')\n","plt.grid('on')\n","plt.xlabel('features')\n","plt.ylabel('labels')\n","x = np.asarray([[0], [30]])\n","plt.plot(x, regr.predict(x), 'r', linewidth=2.5)\n","plt.show()"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"collapsed":true,"id":"I3oGVFDMse7Q"},"source":["***\n","* The red line in the above plot indicates the trained linear model, while the blue points indicate the training data points. Would you say that the model does a good job of modeling the pattern in the training data?\n","\n","***\n","* Next, let's see how good the model is at predicting the labels for new test points, that were not seen during the training. After all, this is the behaviour that is of most concern from a practical point of view!\n","\n","* Prediction for unseen data is performed using the object \"regr\" as well. (Actually, while plotting line we already used this function \"regr.predict\".)\n","\n","* First, let us read some unseen data from file, predict the labels and plot them. "]},{"cell_type":"code","metadata":{"id":"fgq6zO4Cse7Q"},"source":["# read test data inputs / features\n","test_features = np.loadtxt('machine_learning/data/test_features_linear_regression.txt')[:,np.newaxis]\n","print(\"Test sample's features:\\n {}\".format(test_features))\n","\n","# use the predict function of the object to predict for a new set of samples\n","test_predict = regr.predict(test_features)\n","print(\"Predicted labels:\\n {}\".format(test_predict))\n","\n","# plot the predicted labels for the test features\n","plt.scatter(test_features, test_predict, color='g', s=100)\n","plt.grid('on')\n","plt.xlabel('features')\n","plt.ylabel('labels')\n","x = np.asarray([[-0], [30]])\n","plt.plot(x, regr.predict(x), 'r', linewidth=2.5)\n","plt.show()"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"c7PbQOf7se7Q"},"source":["***\n","* Models are often not perfect.\n","* We have also seen in the training data that there is a discrepancy between model line and labels of the data.\n","* Such a discrepancy will also exist in the test set. \n","* Let us now read the \"true\" labels of the test set and visualize the difference with the model predictions. "]},{"cell_type":"code","metadata":{"id":"fJ_vUHnZse7Q"},"source":["# read the true labels of the test data\n","test_labels = np.loadtxt('machine_learning/data/test_labels_linear_regression.txt')\n","\n","# plot the true labels along with the predicted labels to visualize their discrepancy\n","plt.scatter(test_features, test_predict, color='g', s=100)\n","plt.scatter(test_features, test_labels, color='k', s=100)\n","plt.grid('on')\n","plt.xlabel('features')\n","plt.ylabel('labels')\n","x = np.asarray([[-0], [30]])\n","plt.plot(x, regr.predict(x), 'r', linewidth=2.5)\n","plt.show()"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"6UppjEKYse7R"},"source":["***\n","* We can also quantify the discrepancy between model prediction and \"true\" labels using the same cost function as we used in the training part: \n","$\\sum_{n=1} ( y_n - (ax_n+b) )^2$"]},{"cell_type":"code","metadata":{"id":"87o8jXZYse7R"},"source":["total_test_error = np.sum((test_labels - test_predict)**2)\n","mean_squared_error = np.mean((test_labels - test_predict)**2)\n","root_mean_squared_error = np.sqrt(np.mean((test_labels - test_predict)**2))\n","print(\"Total test error: {}\".format(total_test_error))\n","print(\"Mean squared error: {}\".format(mean_squared_error))\n","print(\"Root mean squared error (RMSE): {}\".format(root_mean_squared_error))"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"mxSZuvGBse7R"},"source":["***\n","* Phew! So finally we reach the end of the toy linear regression example. To summarize, we did the following steps:\n","   * read the training features and their corresponding labels.\n","   * visualized the training data.\n","   * created an instance of the machine learning model to be fit to the training data.\n","   * solved an optimization problem to fit the model to the training data.\n","   * visualized the trained model and it's performance on the training data.\n","   * read the features of the test data and used the trained model for doing predictions on new test features, that were not part of the training dataset.\n","   * compared the predictions with the true labels of the test features.\n","***\n","* If you would like to, scroll back to the top of the notebook and ensure that you understand where and how each of these steps is being done.\n","\n","***\n","***\n","<h2> <u> Exercise 2:</u></h2>\n","\n","* In this exercise, you will\n","  * Read new training data: features (from data/ex2_features_regression.txt) and labels (from data/ex2_labels_regression.txt) and visualize the same. \n","  * Fit a linear regression model to the training data. Please name the linear regression object differently, e.g. regr_ex.\n","  * Visualize the trained model.\n","  * Read features of test samples (from \"data/ex2_test_features_regression.txt\") and predict labels for the same.\n","  * Read \"true\" labels of the test samples (from data/ex2_test_labels_regression.txt), compare the predicted values with real labels and compute RMSE. "]},{"cell_type":"code","metadata":{"id":"tAH9Jk6fse7S"},"source":["# TODO\n"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"BxmyGiPgse7S"},"source":["* Would you say that the linear model does a good job in this new regression task that you solved in the exercise?\n","* If not, how else could the data be modelled? With a quadratic function, perhaps? Which parts of the code will have to change in order to do this? You need not implement this, but it would instructive to simply figure out which parts of the code need to change.\n","***\n","***\n","***\n","\n","<h2> Binary Classification with Logistic Regression </h2>\n","\n","* Okay, let's now digress from regression!\n","* In the second task, we will focus on is binary classification.\n","* We will use Logistic Regression for this task. \n","\n","***\n","* Logistic regression also has a simple model: $y = \\sigma(ax + b)$, where $y$ represents labels (in this case, binary), $x$ represent features and $\\sigma(\\cdot)$ represents the sigmoid function: $\\sigma(w) = \\frac{1}{1 + e^{-w}}$. \n","\n","* The output of the <a href=\"https://en.wikipedia.org/wiki/Sigmoid_function\">sigmoid function</a> is in the range [0, 1]. Thus, the predictions are considered as probabilities, i.e. $p(y=1|x) = \\sigma(ax+b)$.\n","\n","***\n","* In the same formulation we can also consider the case where each input data point has multiple features, e.g. $x_1$ and $x_2$.\n","* In this case, the only difference is that the product $ax$ becomes a vector product: $a\\cdot x = a_1x_1 + a_2x_2$. Accordingly, the model becomes: $y = \\sigma(a\\cdot x + b)$.\n","* Logistic regression also has two parameters $a$ and $b$. The parameter $a$ will be a vector of the same size as the number of features for each data point.\n","\n","***\n","* Let us focus on a specific dataset: "]},{"cell_type":"code","metadata":{"id":"vCmRLEjnse7S"},"source":["# read features and corresponding labels\n","features = pd.read_csv('machine_learning/data/features_linear_classification.csv')\n","labels = pd.read_csv('machine_learning/data/labels_linear_classification.csv')\n","\n","# visualize the read data\n","pos_rows = labels['0'] > 0\n","neg_rows = labels['0'] <= 0\n","plt.plot(features.feature1[pos_rows],features.feature2[pos_rows],'+',markersize=10,mew=2)\n","plt.plot(features.feature1[neg_rows],features.feature2[neg_rows],'_',markersize=10,mew=2)\n","plt.grid('on')\n","plt.xlabel('feature1',fontsize=16), plt.ylabel('feature2',fontsize=16)\n","plt.show()\n","\n","# convert Pandas dataframe into numpy arrays: \n","features = features.values\n","labels = labels['0'].values"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"V4SWCoqGse7T"},"source":["***\n","* Logistic regression is in the same module as linear regression in the scikit-learn package.\n","* We create the necessary object and train the model with the available data.\n","\n","<a href=\"https://scikit-learn.org/stable/modules/generated/sklearn.linear_model.LogisticRegression.html#sklearn.linear_model.LogisticRegression\">LogisticRegression documentation</a>"]},{"cell_type":"code","metadata":{"id":"0Nv76BLMse7T"},"source":["# We create an object that can do logistic regression\n","clas = linear_model.LogisticRegression()\n","# We use the data to estimate its parameters with the fit function\n","clas.fit(features, labels)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"eRsRxBRAse7T"},"source":["***\n","* There are several things to note here: \n","\n","  * First, the creation of the logistic regression object and training is done exactly the same way as in linear regression. This extends to almost all models in the scikit-learn package. **More importantly, this also extends to almost all machine learning algorithms conceptually. Once you have the data, you determine the parameters of the model that best predicts labels from features in the training data.**\n","\n","  * The differences are under the hood:\n","    * Models differ: so far, we have only seen linear models. There are other more complex models, as we will see in the next notebook.\n","    * Costs functions (which are optimized to obtain the model parameters) differ.\n","   \n","***\n","* The main cost function that got minimized for the logistic regression is the **cross-entropy**:\n","\n","$\\arg_{a,b}\\min \\sum_{n=1} y_n \\ln \\hat{y}_n + (1 - y_n)\\ln (1 - \\hat{y}_n)$, where $\\hat{y}_n=\\sigma(a\\cdot x_n + b)$.\n","\n","***\n","* Let's visualize the learned model. To do this, we will look at the decision boundary between the two categories. \n"]},{"cell_type":"code","metadata":{"id":"GgeGO992se7T"},"source":["# plot the training data\n","pos_rows = labels > 0\n","neg_rows = labels <= 0\n","plt.plot(features[pos_rows,0],features[pos_rows,1],'+',markersize=10,mew=2)\n","plt.plot(features[neg_rows,0],features[neg_rows,1],'_',markersize=10,mew=2)\n","plt.grid('on')\n","plt.xlabel('feature1',fontsize=16), plt.ylabel('feature2',fontsize=16)\n","\n","# overlay the decision boundary\n","x = np.asarray([[-20], [25]])\n","# coefficients of the logistic regression are saved in the \"clas\" object and can be constructed into a line as\n","m = clas.coef_[0,0] / clas.coef_[0,1]\n","b = clas.intercept_ / clas.coef_[0,1]\n","plt.plot(x[:,0], b - m*x[:,0], 'r--', linewidth=2)\n","plt.show()"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"5UtSKYa7se7T"},"source":["***\n","* As before, let us now read features of test samples and perform prediction."]},{"cell_type":"code","metadata":{"id":"UT83scrAse7U"},"source":["# read test features\n","test_features = np.loadtxt('machine_learning/data/test_features_linear_classification.txt')\n","print(\"Test sample's features:\\n {}\".format(test_features))\n","\n","# use the predict function of the object to predict for a new set of samples\n","test_predict = clas.predict(test_features)\n","print(\"Predicted labels:\\n {}\".format(test_predict))\n","\n","# plot\n","x = np.asarray([[-20], [25]])\n","m = clas.coef_[0,0] / clas.coef_[0,1]\n","b = clas.intercept_ / clas.coef_[0,1]\n","plt.plot(x[:,0], b - m*x[:,0], 'r--', linewidth=2)\n","plt.grid('on')\n","plt.xlabel('feature1',fontsize=16), plt.ylabel('feature2',fontsize=16)\n","plt.plot(test_features[:,0],test_features[:,1],'p',markersize=10,mew=2)\n","plt.show()"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"kmwnoxNtse7U"},"source":["***\n","* There is an important point to notice: **Prediction is performed in the same way as we have done in the linear regression case.*** \n","\n","* As in the linear regression case (and as in most cases), models are not perfect and will make errors when predicting.\n","\n","* We can visualize this by looking at the \"true\" labels of the test samples.\n","\n","* Let us visualize this first and then quantify the error in terms of \"classification accuracy\"."]},{"cell_type":"code","metadata":{"id":"L4XrBepkse7U"},"source":["# read true labels\n","test_labels = np.loadtxt('machine_learning/data/test_labels_linear_classification.txt')\n","\n","# plot\n","x = np.asarray([[-20], [25]])\n","m = clas.coef_[0,0] / clas.coef_[0,1]\n","b = clas.intercept_ / clas.coef_[0,1]\n","plt.plot(x[:,0], b - m*x[:,0], 'r--', linewidth=2)\n","plt.grid('on')\n","plt.xlabel('feature1',fontsize=16), plt.ylabel('feature2',fontsize=16)\n","\n","# get the indices of the correct and wrong predictions by comparing with the true labels: \n","correct_predictions = np.where(test_labels == test_predict)[0]\n","wrong_predictions = np.where(test_labels != test_predict)[0]\n","plt.plot(test_features[correct_predictions,0],test_features[correct_predictions,1],'pg',markersize=10,mew=2)\n","plt.plot(test_features[wrong_predictions,0],test_features[wrong_predictions,1],'pr',markersize=10,mew=2)\n","plt.show()"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"cBUs7BKwse7U"},"source":["***\n","* In the above plot, the test points that were correctly classified are shown in green, while the red points indicate the misclassified test points.\n","* We see that there are three test samples where logistic regression made the wrong prediction.\n","* We can quantify this using different quantities: "]},{"cell_type":"code","metadata":{"id":"1MiCL2sKse7U"},"source":["class_accuracy = np.sum(test_predict == test_labels) / test_features.shape[0]\n","class_fps = np.sum(test_predict > test_labels) / test_features.shape[0]\n","class_fns = np.sum(test_predict < test_labels) / test_features.shape[0]\n","print('Classification accuracy: {}'.format(class_accuracy))\n","print('False positive rate: {}'.format(class_fps))\n","print('False negative rate: {}'.format(class_fns))"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"OLowRV5Zse7V"},"source":[""],"execution_count":null,"outputs":[]}]}